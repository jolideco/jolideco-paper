% Define document class
\documentclass[twocolumn]{aastex631}

% Filler text
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{showyourwork}
\usepackage[autostyle]{csquotes}
\usepackage{amssymb}
\let\tablenum\relaxgit 
\usepackage{siunitx}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\arcsinh}{arcsinh}
\newcommand{\chandra}{\textit{Chandra}~}
\newcommand{\xmm}{\textit{XMM}~}
\newcommand{\fermi}{\textit{Fermi}-LAT~}
\newcommand{\jolideco}{\textit{Jolideco}~}
\newcommand{\aposteriori}{a~posteriori~}
\newcommand{\gammaray}{$\gamma$-ray\xspace}
\newcommand{\xray}{X-ray\xspace}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}

\newcommand{\vlk}[1]{{\color{blue} [VLK: #1]}}

% Begin!
\begin{document}

% Title
    \title{Joint Likelihood Deconvolution of Astronomical Images in the Presence of Poisson Noise}

% Author list
    \author[0000-0003-4568-7005]{Axel Donath}
    \author[0000-0002-0905-7375]{Aneta Siemiginowska}
    \author[0000-0002-3869-7996]{Vinay Kashyap} 
    \author[0000-0000-0000-0000]{David van Dyk}


% Abstract with filler text
    \begin{abstract}
        We present a new method for Joint Likelihood Deconvolution (Jolideco) of a set of astronomical observations of the same sky region in the presence of Poisson noise. The method reconstructs a single flux image from a set of observations by optimizing the \aposteriori joint Poisson likelihood of all observations under a patch based image prior. The patch prior is parameterised by a Gaussian Mixture model (GMM) which we trained on astronomical images with high signal to noise ratio, including data from the  James Webb Telescope as well as the GLEAM radio survey. During the reconstruction process the patch prior adapts to the patch structures in the data by finding the most likely GMM component for each patch in the image. By applying the method to simulated data we show that both the combination of multiple observations as well as the patch based prior lead to a much improved reconstruction quality in many different source scenarios as well as signal to noise regimes. We show also that the method yields superior reconstruction quality to alternative standard methods such as the Richardson-Lucy method. We also the results of the method applied to example data from the \chandra observatory as well as the \fermi. In both cases \jolideco improves the angular resolution as well as signal to noise ratio 
        by a factor of 42 \todo{it would be good to summarize the results with a few key numbers...}.
    \end{abstract}

    % Main body with filler text


    \section{Introduction}
    The resolution of any physical or astronomical imaging process is inherently limited by the instrument or telescope used. Furthermore, the quality of the resulting image is affected by background or instrumental measurement noise and non-uniform exposure. This is especially true in the low signal-to-noise regime, where images are affected by Poisson noise. This includes images taken by \xray and \gammaray~telescopes but also medical images obtained with methods such as positron emission tomography or single-photon emission computed tomography.

    To maximise the scientific use of available data it is desirable to correct for the instrumental imprint on the data as well as reduce its noise level. The process of improving the resolution of an image in post-processing is often called deblurring or deconvolution, as it aims to invert the effect of the data being degraded by a forward convolution through the point spread function (PSF) of the instrument. In general this class of problems is categorised as \textit{inverse problems}. If there is prior information available on the shape of the PSF, we typically refer to the process more precisely as unblind deconvolution. In case of astronomical observations the PSF can for example be estimated empirically from observation of known point sources, such as distant activate galactic nuclei or a model of the PSF can obtained from simulation of the telescope or instrument.

    \subsection{Related Work}
    In literature there have been many efforts to solve the unblind deconvolution problem for images affected by Poisson noise. Most methods rely fundamentally on modeling the image generating process as a linear process. Given a known point spread function a prediction of the discrete measured image can be obtained via \textit{forward folding} and be compared against an observation assuming Poisson statistics per pixel. However the methods differ in their additional prior or regularisation assumptions as well as their approach to optimization or sampling of the underlying likelihood function.
    
    The standard baseline method for image deconvolution was proposed independently by \cite{Richardson1972} and \cite{Lucy1974} (we will further refer to this method as \textit{RL}). The method involves estimating the original image by iteratively convolving the observed image with the PSF and dividing by the result of convolving the estimated image with the transposed PSF. The iterations continue until the estimated image converges to a satisfactory solution. 

    However in practice the RL method was found to have multiple, partly related limitations: first there is no objective criterion for when to stop the optimization process. Usually the iteration is stopped when the reconstructed image seems plausible to the eye. Second, when the number of iterations is too high, the method shows a tendency to decompose regions of constant brightness into a set of bright emission peaks (\enquote{speckles}). This problem was e.g. described and studied by \cite{Reeves1995} and \cite{Fish1995}. Lastly the RL method does not provided an associated estimate of the uncertainties of the reconstruction, which is desirable especially in the Poisson domain to measure the significance of specific features found in the reconstructed image.
    
    To address the question of when to stop the RL iteration \cite{Reeves1995} proposed to use cross validation. The authors of \citep{Bi1994} proposed a bootstrapped based assessment of the significance of reconstructed image as as a stopping rule for RL.
    
    To address the issue with RL decomposing extended emission structures into groups of point source, multiple additions to the RL method were proposed. Most importantly this lead to the introduction of regularised RL methods, such as use of total variation \citep{Dey2006} or sparseness \todo{add reference}. The challenge is due to fundamentally contradicting goals: deconvolution favoring point sources will decompose extend structures, while introducing local correlations will lead to less sharp reconstrcution of point sources.

    Multi-scale LIRA prior has been used in image deconvolution to incorporate error estimates and Bayesian sampling. This prior has been used in various studies, including those by \cite{Esch2004} and \cite{Connors2011}. However, the prior is ad-hoc and lacks physical information or assumptions, limiting its applicability.

    Another approach to image deconvolution is joint likelihood RL, which has been shown to be effective in creating a merged image from multiple images \cite{Ingaramo2014}. In this approach, RL takes the best out of each image and merges them to create a more accurate representation of the object.

    \cite{Pumpe2018} and \cite{Selig2015} approached the problem of deconvolution of Poisson noise data implicitly using the framework of information field theory. Deconvolution, Denoising and Decomposition (D3PO) and (D4PO). 
    
    Recently, image deconvolution using physical assumptions and decomposition into point and diffuse flux in 3D/4D has gained attention. proposed priors based on physical assumptions, which have been shown to work well for large-scale emission. However, the smoothness assumption in these methods can lead to artifacts in the galactic plane.
    
    Interesting approaches to the problem of deconvolution can of course be found in the field of computer vision. For example \cite{Zoran2011} build on very similar assumption of local correlation for natural images. Instead of assuming a fixed local correlation structure, such as smoothness defined by a Gaussian with a correlation size, they proposed to learn the local correlation structure on millions of patches extracted from a set of reference images. This approach can be seen as a form of transfer learning. For this they proposed to split training images from representative data sets into small patches of size 8x8~pixels and fit a 64 dimensional Gaussian Mixture Model (GMM) to the distribution of extracted patches. Each pixel of the patch 
    is thereby treated as an independent dimension in the model. In total
    they chose to use $k=200$ Gaussian components. To use the GMM in reconstruction they introduce the expected patch likelihood (EPLL). They showed that the patch based GMM prior led to much improved image reconstructions for multiple inverse problems such as denoising, deblurring and inpainting. \cite{Bouman2016} later adapted the patch prior successfully for the reconstruction of radio data from the Event Horizon Telescope (EHT).
    
    
    % \begin{figure*}
    %     \script{rl_decomposition.py}
    %     \begin{centering}
    %         \includegraphics[width=\textwidth]{figures/richardson-lucy-decomposition.pdf}
    %         \caption{
    %             Decomposition of the RL algorithm for various number of iterations. \todo{Improve figure}
    %         }
    %         \label{fig:rl_decomposition}
    %     \end{centering}
    % \end{figure*}

    \subsection{Some Remarks on Deep Learning Methods}
    In the past decade Machine learning (ML) especially deep convolutional neural networks (CNN) became the \textit{de facto} standard for deblurring and super-resolution of natural images \todo{reference review?}. However their success relies on the availability of massive sets of training data with typically low noise and corresponding ground truth to perform supervised learning. For a variety of reasons, which we will outline in the following, this approach is not ideal for astronomical x-ray and \gammaray data.
    
    First astronomical x-ray and \gammaray data data is rare, expensive to obtain and there is no ground truth, because we can only observe the sky with our given instruments. To circumvent this one could rely on transfer learning and simulated data. For example radio or optical images could be forward folded with a known PSF model and be degraded with Poisson noise to generate training data. However astronomical images still have a large dynamic range and show a high diversity in morphological structure, which would still require a large amount of training data. However this approach has been successfully applied for specific classes of sources, such as images of Galaxies in the Sloan Digital Sky Survey (SDSS) \citep{Schawinski2017} or as a post processing step of results obtained with regularised RL \citep{Akhaury2022}.

    In addition to the challenge of obtaining training data, it is known that the PSF of instruments such as \chandra is highly variable. It varies with observation conditions and depends e.g. on energy as well as offset angle from the pointing direction. To cover the whole parameter space of the PSF one would need to train either a network for each of the PSF models, or define a very large network, which would require a very large computational effort.

    Some of the challenges of deblurring with CNN are due to the convolutional nature of the networks. In a single layer, linear approximation, the network would need to learn the inverse filter of the PSF, corresponding to the well known \textit{Wiener filter} solution of the problem. This requires large convolution kernels or deep network architectures, which require in turn large amounts of training data. This challenge was described early by \cite{Li2014} who introduced a baseline architecture for a deep CNN for image deblurring.
    
    One last important challenge for deep learning based methods is obtaining uncertainty estimates for the reconstructed image from CNNs. In the limit of Poisson noise it is desirable to trace the full a-postiori likelihood, which is not possible for most network architectures, with the exception of for example normalising flows.
    This problem could be adresses with Monte Carlo base methods, such as bootstrapping, further increasing the demand on training data and computation ressources.

    \section{Method}
    \subsection{Poisson Joint Likelihood}
    Our goal is to recover an image $\mathbf{x}$ from a set of multiple low counts observations. Most generally we assume our total dataset consists of $J$ individual observations of the same region of the sky, which we jointly model. The assumption is that the underlying, unknown \textit{true} emission image, we are interested in, does not change with time. Under this assumption these datasets can be for example:

    \begin{itemize}
        \item Different observations of one instrument or telescope at different times and observation conditions. E.g., multiple observations of \chandra of the same astrophysical object with different offset angles and exposure times
        \item Observations from different telescopes, which operate in the same wavelength range. E.g., a \chandra and \xmm observation of the same region in the sky
        \item A single observation of one telescope with different data quality categories and different associated instrument response functions, such as event classes for \fermi
    \end{itemize}

    Or even an arbitrary combination of the possibilities listed above. It could also be the combination of observations in different energy ranges, however in this case the assumption of the morphology not changing with energy would apply, which is rarely fulfilled for the astronomical data we are dealing with.
    
    For each individual observation $j$ the predicted counts can be modelled by \textit{forward folding} the unknown flux image $\mathbf{x}$ through the individual instrument response:
    %
    \begin{equation}
        \label{eq:model}
        \boldsymbol{\lambda}_j(\mathbf{x}) = \mathrm{PSF}_j \circledast \left(\mathbf{E}_j \cdot (\mathbf{x} + \mathbf{B}_j) \right)
    \end{equation}

    Where the expected counts $\lambda_j$ are given by the convolution of the unknown 
    flux image $\mathbf{x}$ with the observation specific point spread function $\mathrm{PSF}_j$. Additionally
    an observation specific image of the exposure $\mathbf{E}_j$ and background emission $\mathbf{B_j}$ can be
    optionally taken into account.

    Given a single observation $j$ of an unknown flux image
    $\mathbf{x}$ and assuming the noise in each pixel $i$ in the recorded counts image
    $\mathbf{D_j}$ follows a Poisson distribution, the likelihood $\mathcal{P}$
    of obtaining the measured image from a model image of the expected
    counts $\boldsymbol{\lambda_j}$ with $N$ pixels is given by:
    %
    \begin{equation}
        \label{eq:poisson}
        P\left( \mathbf{D_j} | \boldsymbol{\lambda_j} \right) = \prod_{i=1}^N \frac{{e^{ - D_{j,i} } \lambda_{i,j} ^ {D_{j,i}}}}{{D_{j,i}!}}
    \end{equation}

    By taking the logarithm and dropping the constant terms one can transform the
    product into a sum over pixels, which is also often called the \textit{Cash}
    \citep{Cash1979} fit statistics:
    %
    \begin{equation}
        \label{eq:cash}
        \mathcal{C}\left( \mathbf{D_j} | \boldsymbol{\lambda_j} \right) = \frac{1}{N}\sum_{i=1}^N \lambda_{j,i} - D_{j, i} \log{\lambda_{j,i}}
    \end{equation}

    To account for the possibility of different datasets having a different number of pixels we divide by $N$ (\todo{Does this need more justification...?}).
    By summing over all observations we finally get the joint log-likelihood of measuring a set of counts images $\mathbf{D}$ given an unknown flux image $\mathbf{x}$:
    %
    \begin{equation}
        \label{eq:joint}
        \mathcal{L}\left( \mathbf{D} | \mathbf{x} \right) = \sum_{j=1}^J \mathcal{C}\left( \mathbf{D_j} | \mathbf{x} \right)
    \end{equation}

    Using Maximum Likelihood Estimation (MLE) we could try to get an estimate $\hat{\mathbf{x}}$ directly,
    using some optimization procedure. However in general this represents an \textit{ill posed}
    inverse problem. Estimates for $\mathbf{x}$ which have similar likelihood values $\mathcal{L}(\hat{\mathbf{x}_1})$ look very different from each other and might also look different compared to a given ground truth, if available. 
    Both the convolution operation as well as the level of noise lead
    to a loss of information in the data which cannot easily be recovered.
    The likelihood surface shows many local, close-by minima. Depending on the initial estimate for $\mathbf{x}$ and the minimization algorithm the MLE based estimate
    is likely to only find such a local minimum. This is fundamentally the reason for RL decomposing estimates into point sources, described in the previous section. 
    
    \todo{did you know this sum corresponds to the loop over data sets in machine learning...?}

    \subsection{A Posteriori Joint Likelihood}
    When reconstructing images in the context of inverse problems we are operating in a high dimensional parameter space, because each pixel in the image represents an independent parameter in the optimization process. By using prior information we can introduce correlations between the parameters, to reduce the \textit{effective} dimensionality of the problem and guide the optimization process towards unique and more stable solutions.
    
    Using Bayes rule we can estimate the \aposteriori likelihood under a given prior:
    %
    \begin{equation}
        \label{eq:bayes}
        P(\mathbf{x}|\textbf{D}) = P(\mathbf{x} ) \frac{P(\textbf{D} |\mathbf{x})}{P(\textbf{D})}
    \end{equation}

    And use a Maximum A Posteriori (MAP) approach to find an estimate for $\mathbf{x}$. Taking the logarithm of Eq.~\ref{eq:bayes}, replacing the definition of the likelihood $P(\mathbf{D}|\mathbf{x})$ with Eq.~\ref{eq:joint} and dropping the normalisation term $P(\mathbf{D})$ which is independent of $\mathbf{x}$, we get the following expression for the log-posterior likelihood $\mathcal{L}$ :
    %
    \begin{equation}
        \label{eq:total}
        \mathcal{L}\left(\mathbf{x} | \mathbf{D} \right) = \sum_{j=1}^J \mathcal{C}\left( \mathbf{D}_j | \mathbf{x} \right) - \beta \cdot \mathcal{P}(\mathbf{x})
    \end{equation}

    Where $\mathcal{C}\left( \mathbf{D_j} | \mathbf{x} \right)$ represents the summed log-likelihood for an individual observation $j$. Additionally the function includes a log-prior term $\mathcal{P}(\mathbf{x})$ and a factor $\beta$ to adjust the weight of the prior with respect to the joint log likelihood term. The parameter $\beta$ can be seen as a hyper-parameter, chosen by the user to quantify their belief in the correctness of the chosen prior distribution. \todo{this is somewhat dissatisfying...but in practice very common...see e.g. any kind of regularisation...if there is enough data cross validation could be used to tune this hyper-parameter}.

    \subsection{Patch Based Image Prior}
    In general it is difficult to capture global image statistics in a prior likelihood taking into account all pixel to pixel correlations. The size of the corresponding correlation matrix would grow with the number of pixels squared and thus quickly becomes computationally untraceable. However astronomical images typically only show local correlations between pixels. This is because the spatial scale of astrophysical processes in an image is always limited by the finite interaction time and the fact that the interaction cannot propagate faster than the speed of light (structure in the CMB might be an exception here). This means on smaller scales astronomical images often contain basic structures such as edges, corners, filaments or periodic patterns formed by local astrophysical processes. For this reason we adopt the idea of a patch based image prior approach as introduced by \cite{Zoran2011}. Given a pre-trained Gaussian Mixture Model (GMM) for the distribution of pixel values in patches, we can evaluate a patch based prior for the reconstructed image $\mathbf{x}$ and approximate the \aposteriori likelihood.
    
    One convenient property of the GMM is the possibility to evaluate its likelihood in closed form. For a given single patch $\mathbf{x}_n$ it is given by:
    %
    \begin{equation}
        \label{eq:gmm}
        P_{GMM}(\mathbf{x}_n) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_n| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k^2)
    \end{equation}

    Where $\mathcal{N}$ is a Gaussian distribution, $K$ the total number of components, $\boldsymbol{\mu}_k$ the vector of means for a given component $k$, $\boldsymbol{\Sigma}_k$ the corresponding covariance matrix and $\pi_k$ the weight of the component. The different weights are required to sum to unity such that $\sum_{k=1}^N\pi_k = 1$. 

    From Equation~\ref{eq:gmm} we can also derive the log-likelihood for a specific choice of the component $k$:
    %
    \begin{equation}
    \begin{split}
    \mathcal{P}_{GMM}(k| \mathbf{x}_n) = -\frac{1}{2} \left[ \right. d \log(2\pi)
    + \log(|\boldsymbol \Sigma_{k}|)\\
    + (\mathbf{x}_n - \boldsymbol{\mu}_{k})^T \Sigma^{-1}_{k}(\mathbf{x}_n - \boldsymbol{\mu}_{k}) \left. \right]
    \end{split}
    \end{equation}

    If we split the image into non-overlapping patches, we can find for each noisy patch $\mathbf{x}_n$ the GMM component $\hat{k}_n$, which maximises the log-likelihood of the GMM:
    %
    \begin{equation}
        \hat{k}_n = \underset{k \in \{1, ..., N\} }{\argmax}{\mathcal{P}_{GMM}(k| \mathbf{x}_n)}
    \end{equation}
    \vspace{0.2em}

    As the number of components in the GMM is limited we can do this simply by evaluating the likelihood for each of the components and choose the one with the maximum value of the log-likelihood. To finally evaluate the log-likelihood of the total image we can sum up the maximum log-likelihood values of the individual patches: 
    %
    \begin{equation}
        \mathcal{P}(\mathbf{x}, \mathbf{k}) = \sum_{n = 1}^N \mathcal{P}_{GMM}(\hat{k}_n | \mathbf{P}_n \mathbf{x})
    \end{equation}

    Where $\mathbf{P}_n$ is a matrix that extracts the $n$-th patch
    from the image $\mathbf{x}$ to be reconstructed. This way the index $k$ that identifies the most likely GMM component per patch, becomes a discrete hyper-parameter in the model that is optimized along with the image $\mathbf{x}$. 
    
    In the case of non-overlapping patches the 
    
    \begin{itemize}
        \item the case of non-overlapping patches is rather simple
        \item for overlapping patches we have to make a few more approximations
        \item either we neglect the correlation between overlapping patches
        \item or we have to optimize the array of $\mathbf{k}$ at the same time, instead of per patch optimization. However optimization on discrete 
        values is not easily possible in Pytorch. However here is one idea:
        group the GMM components by similarity and introduce a single continues
        parameter $z$, which interpolates between similar components.
        \item Another idea is to use Gumbel Softmax...
        \item we also compute the mean of the likelihood for the overlapping parts
    \end{itemize}

    To use the learned GMM as prior for the reconstruction of another image, the image is split into overlapping patches. For each of the patches the log-likelihood for each of the GMM components is evaluated. The resulting grid of overlapping patches is illustrated in Figure~\ref{fig:patches}.

    \begin{figure}
        \script{patches.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/patches.pdf}
            \caption{
                Grid of overlapping patches of size 8x8 pixels. The overlap size was chosen to be 2 pixels.
            }
            \label{fig:patches}
        \end{centering}
    \end{figure}

    with radio astronomy data. They have shown that the reconstructed image only weakly depends on the choice of the reference data on which the the patch prior is learned. They found equivalent results for GMMs learned on natural images and specifically simulated images of black hole ring structures.

    Using half quadratic splitting and the Wiener filter solution, which is not possible in our case. Instead we choose a direct optimization by relying on differentiable programming later. 
    

    \subsubsection{Patch Normalisation and Dynamic Range}
    Astronomical images typically show a much higher dynamic range compared to natural images. This is mostly due to the existence of point sources, which rarely occur in natural images. Astronomical point sources typically relate to compact bright objects, such as active galactic nuclei at high distances, or binary objects and stars at Galactic distances. In addition the visible matter in the universe is sparse such that astronomical images typically show sparse bright emission on a dark background. 

    To normalise the patches \cite{Zoran2011} proposed to subtract the patch mean before learning the GMM. This leads to the GMM modeling the same morphological features at different levels of contrast as different components. While this proved to be beneficial in the context of reconstructing natural images it does not scale to images with high dynamic range, only at the cost of introducing a lot more components to the GMM, which is unfeasible in practice due to limited availability of training data and computational resources. For this reason we propose the following modified normalisation for the patches:
    %
    \begin{equation}
        \label{eq:patch-norm}
        \tilde{\mathbf{x}}_n = \frac{\mathbf{x}_n - \Bar{\mathbf{x}}_n}{\Bar{\mathbf{x}}_n}
    \end{equation}
    
    With dividing by the mean of the patch $\Bar{\mathbf{x}}_n$ the structure in the patch becomes independent of the total brightness the patch contains. This results in patches being grouped only by their pixel to pixel correlation structure and not by total contrast. This corresponds the assumption that the structure in the patch is statistically independent of the total flux contained in the patch, which we consider a valid assumption for astronomical images.
    
    
    % The high dynamic range of astronomical images challenges any deconvolution method. To recover point sources accurately it is required to keep correlations with neighbouring pixels low, while for extended source the opposite is true. Ideally those contradicting requirements can be handled with a single flexible prior assumption, which adapts to the data, such as the patch based GMM prior introduced in the previous section.

    % With a given non-linearity:

    % \begin{equation}
    %     f(x) = \arcsinh{x / \alpha}
    % \end{equation}

    % \todo{The patch normalisation remains to be decided, but it would be advantageous to get rid of the hyper-parameter associated with the total image normalisation}.
    % The GMM patch prior is learned from other images whose pixel intensities
    % have no physical relationship of the intensities of the Poisson data we are
    % dealing with. To overcome this the GMM is learned on normalized images,
    % where the intensity values are constrained between 0 and 1.
    % To evaluate the GMM patch prior on the 
    % model image $\mathbf{x}$ the pixel values of $\mathbf{x}$  need to be mapped 
    % in the same dynamic range between 0 and 1. Multiple common choices for such a
    % mapping function are shown in Fig.~\ref{fig:image-norms}. 


    % In practice we are interested in the likelihood of a given structure in a patch and
    % So we subtract the mean value of a given patch. This represents the assumption that
    % the image structure is independent of the total flux contained in the patch.
    

    % % \begin{figure}
    % %     \script{image-norms.py}
    % %     \begin{centering}
    % %         \includegraphics[width=\linewidth]{figures/image-norms.pdf}
    % %         \caption{
    % %             Various image norms with different dynamic behaviour.
    % %         }
    % %         \label{fig:image-norms}
    % %     \end{centering}
    % % \end{figure}

    % The choice of the image normalisation allows to adjust the contrast of the image
    % and e.g. enhance low intensity structures, which also enhances the regularising
    % effect of the patch prior. 

    % \todo{Which normalisation to choose, atan, asinh, log, inverse-cdf?}

    % There are arguments for $\arcsinh(\mathbf{x})$, see e.g. \cite{Lupton2004}, which we use.

    \subsubsection{Cycle Spinning}
    To avoid artifact due to the choice of the grid of overlapping patches we propose three variations:

    \begin{itemize}
        \item Cycle spinning by randomly shifting the image by a given number of pixels in x and y direction, e.g. used in \cite{Esch2004}
        \item Sub pixel cycle spinning, by randomly distributing the brightness of a given pixel to a 4 pixel grid \todo{This is something I came up with to allow for point sources to have sub-pixel positions, but I never showed it actually works...}
        \item A randomly chosen patch grid, such that each pixel is at least covered once. The random grid of patches was proposed by \cite{Parameswaran2018}. This has the advantage of potentially speeding up the computation as well. \todo{It is available in the implementation, but a bit ad hoc, maybe just not mention...}
    \end{itemize}

    \subsection{Choice and Tuning of Hyperparameters}
    \subsubsection{Cross Validation}
    Cross validation (CV) is an established method for model selection in statistics and machine learning to prevent a model from overfitting to the training data. To perform CV the available data is slit into a \enquote{training} and \enquote{validation} dataset. While optimizing the log-likelihood value is monitored  The use of multiple dataset allows to use cross-validation to prevent overfitting. One of the standard techniques in machine learning...

    Cross validation is feasible once sufficient data is available, which is e.g. the 
    case for deep \chandra~observations.

    CV as stopping rule for RL was proposed by \cite{Reeves1995}...

    \subsubsection{Other Approaches}

    Self supervised Poisson loss? J-invariance has been shown to work, Grid Search? Calibration on similar data with ground truth?
    

    \subsection{Systematic Errors of Predicted Counts}
    Each measured counts image $\mathbf{D_j}$ is associated with systematic errors. This includes e.g. the pointing accuracy of the telescopes: when repeatedly pointed to the same object or when using different types of telescopes for observation, the absolute position of an astronomical object might vary in the image between different observations. The process of correcting for these inaccuracies is called \textit{astrometry} and a standard procedure in for example \chandra data analysis. A second example of systematic errors is the variation of background emission between multiple observations.
    
    To account for both systematic effects we extend the model for the predicted counts introduced in Eq.~\ref{eq:model}, by a background normalisation factor $\alpha_j$ and a linear shift in position $\phi_j(\mathbf{x}| \delta_{x,j}, \delta_{y,j})$, which is specific for each observation. This yields the following modified model definition:
    %
    \begin{equation}
        \label{eq:model-npred-calibration}
        \mathbf{\lambda}_j = \mathrm{PSF}_j \circledast \left(\mathbf{E}_j \cdot (\phi_j(\mathbf{x}| \delta_{x,j}, \delta_{y,j}) + \alpha_j \cdot \mathbf{B}_j) \right)
    \end{equation}
    
    In practice one observation is used as a reference and its corresponding systematic shifts in position $\delta_{x,j}, \delta_{y,j}$ are frozen during optimization. An example of the effect of the positional shift and background normalisation can be seen in Fig.~\ref{fig:npred-systematics}.

    \subsection{Estimation of Statistical Errors}
    Estimating the global covariance matrix for image based data is computationally not feasible, because the size of the matrix scales with the squared number of pixels in the image. However there are a few ways to estimate the statistical uncertainties associated with the flux image reconstructed by \jolideco. We propose three methods for this:

    \begin{itemize}
    \item The first method we propose is bootstrapping of observations. Given that multiple observations of the same region of the sky are available, \jolideco can be run repeatedly on a random subset of those. This results in a set of reconstructed images which are sampled from the posterior distribution. Depending on the size of the images the associated computational effort can be very large as it requires running \jolideco many times to achieve sufficient statistics.

    \item If only one or few observations are available we propose bootstrapping of the events used to compute the counts image instead. This basically corresponds to splitting a single observation into multiple ones splitting the single exposure into multiple time intervals. This way one can apply the first proposed method again.
    
    \item We can also estimate the diagonal of the Hessian. However this is numerically challenging because of the random elements in the method, such as stochastic gradient decent and cycle spinning.
    
    \end{itemize}

    \todo{One could try to estimate a sparse covariance matrix...or approximate it using the GMM covariance...}

    \section{Implementation}
    \subsection{Jolideco Framework}
    The goal of the reconstruction process is to optimize the a-posteriori likelihood defined by Equation~\ref{eq:total}. Given that each pixel in the reconstructed image represents an independent parameter this represents a high dimensional optimization problem. Differentiable programming frameworks such as \texttt{PyTorch}~\citep{Pytorch2019} allow for solving these kind of high dimensional modeling problems, by using back-propagation and adapted optimization methods for high dimensional models such as stochastic gradient decent.

    We implemented the \jolideco method as an independent Python package, based on \texttt{PyTorch} as optimization back-end. We tried to define a modular, object oriented code structure, to allow parts of the algorithm to be flexible and interchangeable, such as choice of the patch normalisation, the choice of GMM models, optimization methods and serialisation formats for the reconstruction results as well as corresponding diagnostic information such as the trace of the posterior and prior likelihood values. We hope that this simplifies the future improvement and extension of the package. The package is available at \url{https://github.com/jolideco/jolideco}.

    We also use \texttt{Numpy}~\citep{Numpy2020} for handling data arrays and \texttt{Matplotlib}~\citep{Hunter2007} for plotting. To learn the Gaussian Mixture Models from patches we use the standard implementation provided in the \texttt{Scikit-Learn}~\citep{Skimage2014} package. To handle the serialisation of images to the \textit{FITS} data format and to handle world coordinate systems (WCS) we use \texttt{Astropy}~\citep{Astropy2018}. For optimization and internal data handling we use \texttt{PyTorch}.

    \subsection{Jolideco GMM Library}
    \label{ssec:jolideco-gmm-library}
    On radio data \cite{Bouman2016} found the results to be mostly independent of the choice of data the GMM prior was learned on. The strength of the GMM prior is its high \textit{expressiveness}, meaning the prior contains a large variety of patch shapes and adapts to the data in the optimization process. However in the low signal-to-noise regime we typically expect a higher dependence of the results on the choice of the data the GMM was learned on. The choice of the prior becomes more important and to avoid introducing a strong bias it should be adapted to the analysis scenario. For this reason and for purposes of testing we learned and provide a selection of GMMs (a \textit{Library}) to be used with the patch prior for download in a \textit{GitHub} repository\footnote{\url{https://github.com/jolideco/jolideco-gmm-library}}.
    
    \subsubsection{Zoran \& Weiss}
    As baseline reference we use the original GMM provided by \cite{Zoran2011}. The model was learned from the Berkeley image database \citep{Martin2001} which contains natural images and images of every day scenes and objects. The model was learned by splitting the training images into patches of size 8x8 pixels. This resulted in a training set of $2 \cdot 10^{6}$ patches, which were used to fit a GMM with 200 components. The model is provided by the authors as part of the source code for download\footnote{\url{https://people.csail.mit.edu/danielzoran/epllcode.zip}}. More details of how the model was learned are given in the original publication.
    
    \subsubsection{GLEAM Radio Data}
    \label{sssec:gleam-radio-data}
    As an alternative we consider a prior learned from astronomical radio data. For this we relied on high signal to noise radio data from the \textit{GLEAM} survey \cite{HurleyWalker2022}. We downloaded the data using the Gleam VO Client\footnote{\url{https://github.com/ICRAR/gleamvo-client}}. We restricted the choice of data to the Galactic Plane between $b=\pm10\deg$ and $\ell = \pm180\deg$ to avoid a bias towards extra-galactic point sources and represent structures of Galactic sources with rich morphology, such as supernova remnants, pulsar winds etc. The data set still includes Galactic as well as extra-galactic point sources in the background. There is also physical correlation between e.g. the very high energy \gammaray and radio range as their emission is produced by the same electron populations.

    In total we extracted \todo{insert number} patches and trained a GMM with 128 components. We used data augmentation by introducing a random shift of the sub-pixel position.

    \subsubsection{JWST Cassiopeia A}
    The James Webb Space Telescope (JWST) is a large, infrared space telescope launched in late 2021. It delivers image data a very high spatial resolution as well as excellent signal to noise ratio. Both of these properties make JWST data an excellent choice for extracting patches for training a GMM. To limit the data reduction effort we relied on a promotional image of the SNR \textit{Cassiopeia A}\footnote{The image was downloaded from \url{https://webbtelescope.org/contents/media/images/2023/121/01GWQBBY77MHGFV3M3N63KDCEJ}}.
    We first converted the image to gray scale using a perceived brightness model \todo{reference skimage...}. Then we split the image into $approx 2$ million patches and learned a GMM with 128 mixture components.
    
    % \subsubsection{NRAO Jets, Jets, Jets}
    % \todo{Include the jet prior or not?}
    % For the specific application of jet detection and analysis in extract galactic data we learned a GMM patch prior from radio jet images. It represents the prior knowledge of both the morphology and preferred direction of the jet. As reference data we chose
    % a PR image released under the name \textit{Jets, jets jets}  by the NRAO collaboration containing ~150 example jets from their data\footnote{\url{https://public.nrao.edu/gallery/quicklook-samples/}}.

    % We first split the compiled image into 150 individual examples. For each image we ran a blob detection algorithm and line fit to find the preferred direction of the jet. Using this information we aligned all images to the same horizontal reference direction of the jet split all images into patches of size 8x8 pixels. In total we extracted \todo{insert number} patches and used those to train a GMM with 32 components. The prior is useful in a scenario were the direction of the jet known from e.g. previous radio observations. 

    % Alternatively we used a \textit{data augmentation} approach to learn a GMM prior with a randomized direction of the jet. For this we rotated the jet image after alignment by 
    % a random angle. Then again we split each image into patches of size 8x8 pixels. In total we created \todo{insert number} patches and learned a GMM with 64 components. The prior
    % is adapted to the analysis scenario of jet detection, where no prior knowledge on the
    % direction is available.

    \begin{figure}
        \script{gmm-eigen-images.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/gmm-eigen-images.pdf}
            \caption{
                Eigenimages of the covariance matrices of the 32 components of the GMM learned from the NRAO jets. \todo{Provide other examples...?!?}
            }
            \label{fig:gmm-eigen-images}
        \end{centering}
    \end{figure}

    \subsubsection{Chandra SNRs}
    \label{sssec:chandra-snrs}. Because of the limited statistics we did not use any oversampling but kept the pixel
    Astrophysical processes forming the morphological structures are typically self-similar \todo{provide physical argument...I guess it is just the distance distribution?}. Structures appear on multiple angular scales. This means the prior could even be learned from the data itself if sufficiently free of noise. For analysis of Galactic sources with complex morphology and typically radially symmetric features such as supernova remnants (SNRs) we learned a prior from images of the bright \chandra SNRs \textit{Cas A} and \textit{G292.0+01.8}. In total we extracted \todo{insert number} patches and learned a GMM with 128 components.


     
    \subsection{Computational Performance}
    We conduct as series of performance benchmarks (see Appendix?) to asses the scalability of the method to a large number of observations as well as large images. 

    \begin{itemize}
        \item Results are available in \url{https://github.com/jolideco/jolideco-benchmarks}
        \item We tried the binary search tree as proposed in \cite{Parameswaran2018},
        but without success, most likely because the native \texttt{Pytorch} support
        for parallelisation speeds up the standard GMM quite a bit...
        \item Could use a Neural Net for GMM component selection as proposed in \cite{Rosenbaum15}
        \item GPU support via \texttt{Pytorch}

    \end{itemize}

    \subsection{Optimization Strategy}
    Maximising the \aposteriori likelihood defined Eq.\ref{eq:total} with respect to the model parameters $\mathbf{x}$ is a high dimensional optimization problem. To improve the convergence rate of the optimization process we employ the following optimization strategy: the optimization strategy begins with the optimizer viewing $\log{\mathbf{x}}$, and the first step involves running a rough estimate using a uniform prior. This step is optional, but if chosen, the next step involves optimizing the parameters associated with the systematic error described in Eq.~\ref{eq:model-npred-calibration}. Following this, a full optimization is performed using the patch prior, which can be computationally expensive. The optimization algorithm used is ADAM, a stochastic gradient descent method, without decay. Finally, there is an optional step of learning rate scheduling, although so far, a fixed step size value of $\mathrm{lr}=10^{-3}$ has been used.
    
    In practice we employ the following optimization strategy:
    \begin{itemize}
        \item the optimizer sees $\log{\mathbf{x}}$, note this corresponds to an implicit prior of the flux being positive...
        \item First run with uniform prior, to get a rough estimate
        \item (optional) Then optimize the parameters associated to the systematic error described in Eq.~\ref{eq:model-npred-calibration}.
        \item Then we go for a full optimization using the patch prior, which can be computationally costly
        \item we use ADAM (stochastic gradient decent), without decay
        \item  (optional) learning rate scheduling? So far just a fixed step size value of $1e-3$
    \end{itemize}
    
    
        
    \section{Experiments}
    \subsection{Test Datasets}
    \label{subsec:test-datasets}

    To evaluate the reconstruction performance of \jolideco and compare it against other methods we defined a set of test source scenarios. They consist of simplified elementary morphological structures resembling those found in astronomical images. We have devised four distinct arrangements of combinations of point and extended sources of different shapes. The scenarios are:
    
    \begin{itemize}
        \item[(A)] {\enquote{\bf Points}:} point sources arranged in a grid with different separations and brightness. 
        \item[(B)] {\enquote{\bf Asterism}:} a set of four point sources arranged around an extended Gaussian source
        \item[(C)] {\enquote{\bf Shield}:} disk shaped flat extended source with superposed point sources and linear jet-like features 4 different directions.
        \item[(D)] \enquote{{\bf Spiral}:} extended thin double-spiral with a flat disk at the center and point sources adjacent to it. 
    \end{itemize}

    In addition we consider two different instrument scenarios:

    \begin{itemize}
        \item {\enquote{\bf Chandra}:} an instrument with good angular resolution, but low effective area. We assume a Gaussian PSF of sizes $\sigma = 2$ pixels and a reference effective area of \qty[mode=text]{1}{A.U.}.
        \item {\enquote{\bf XMM}:} an instrument with worse angular resolution, but higher effective area. We assume a Gaussian PSF of sizes $\sigma = 6$ pixels and an effective area of \qty[mode=text]{5}{A.U.}.
    \end{itemize}
    
    The ground truth and corresponding expected counts for the combination of scenarios are shown in Figure~\ref{fig:scenarios}, and the details of the locations and brightness of each component is listed in Table~\ref{tab:scenarios}.

    \begin{figure*}
        \centering\includegraphics{figures/scenarios.pdf}
        \caption{Illustration of the expected counts for the different source and instrument scenarios we used to make test datasets. The source patterns shown are for the {\sl (A)}  {\tt points}, {\sl (B) {\tt asterism}}, {\sl (C)} {\tt shield}, and {\sl (D)} {\tt spiral} cases (see Section~\ref{sec:testmodels}). For the instrument scenario \chandra we convolved the with a narrow Gausssian shaped PSF of width $\sigma=2$~pix. For the scenario \xmm we used a Gaussian shaped PSF with a width of $\sigma=2$~pix and a five times increased effective are.
        All images are of size $128{\times}128$.  The dashed red and magenta boxes depict $32{\times}32$ and $64{\times}64$ regions of interest.  Note that the boxes are displaced to the lower left corner in Case B, and leftwards in Case D.
        convolved with a diffuse PSF (Gaussian with $\sigma=6$~pix), \todo{add back boxes if we have time}.
        }
        \label{fig:scenarios}
    \end{figure*}
     
    Constant background levels of $\lambda_{Bkg}= 0.001, 0.01 \textrm{and} 0.1 \textrm{counts/pixel}$. 
    

    \begin{table*}[t]
    \label{tab:scenarios}
      \centering
      \begin{tabular}{l|cr}
        \hline
        Scenario & Description & Parameters \\
        \hline
        A1 & A grid of point sources & x, y \\
        B1-3 & Extended Gaussian shaped source surrounded by point sources & $\sigma$ \\
        C1-3 & Point Sources on top of a disk shaped source, jet like line features & $r_{\mathrm{Disk}}$ \\
        D1-5 & A spiral-arm structure surrounded by point sources and a large ring like feature & 9 \\
        
        
        \hline
      \end{tabular}
      \caption{Source scenarios, \todo{maybe we find a good structure for the table...would list the actual flux values per component}}
      \label{tab:1}
    \end{table*}
    

    \subsection{Methods}
    \label{subsec:methods}
    We compare the performance of \jolideco with different prior assumptions and against the \textit{LIRA} method introduced by \cite{Esch2004}. More specifically we use the following methods:

    \begin{itemize}
        \item \textbf{Pylira}: first we use the \textit{LIRA} method \citep{Esch2004} via the \textit{Pylira} Python wrapper \citep{Donath2022}. We rely on the default prior definition and allow the scale for the background to vary. In total we run the the sampling process for 5000 iterations and use 500 iterations as a \endquote{burn-in} period to allow the MCMC chain to converge. Then we compute the mean of the posterior distribution ignoring the burn-in period.
        
        \item \textbf{Jolideco (Uniform, n=10)}: second we use \jolideco with a uniform prior and 10 iterations. This setup we consider as a baseline method reference similar to RL. While the optimization procedure is different, we still expect to see similar behaviour for the local correlations of pixels. This ad hoc approach of early stopping is often used in practice to regularise the RL method.
        
        \item \textbf{Jolideco (Uniform, n=1000)}: again we use \jolideco with a uniform prior but running for 1000 iterations. Sames as above except without early stopping. This gives rise to the issue of RL decomposing extended emission into point sources. 
        
        \item \textbf{Jolideco (Patch, GLEAM v0.1)}: \jolideco with a patch prior learned on GLEAM data, described in Section~\ref{sssec:gleam-radio-data}. It also uses the standardised patch normalisation as described by Equation~\ref{eq:patch-norm}. We use cycle spinning, a stride of 4 pixels and 2000 \textit{training epochs}. Given 5 observations this results in 10.000 iterations. Which optimizes \enquote{to end} until there is no relevant change in the posterior log-likelihood. In addition we use and upsampling factor of 2.
        
        \item \textbf{Jolideco (Patch, Zoran-Weiss)}: \jolideco with a patch prior from \cite{Zoran2011} and the mean subtracted patch norm. This is for comparison to the original method and the method used on \cite{Bouman2016}. We use maximum image norm with the maximum values defined by the ground truth \todo{is this fair?}.
    \end{itemize}

    We ran the comparison on the whole parameter grid of methods, source scenarios, background levels and instrument scenarios. However in the following we only discuss a subset of the results, which we consider the most relevant. The complete set of results is available at \url{https://jolideco.github.io/jolideco-comparison}.

    \subsection{Results}
    \subsubsection{Comparing Methods}
    %
    \begin{figure*}
        \script{comparison-scenarios.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/comparison-scenarios.pdf}
            \caption{
                Comparison of different deconvolution methods and prior assumptions for different selected source scenarios, as described in Section~\ref{subsec:test-datasets}. The simulation of the data for this figure used a fixed background level of $\lambda_{Bkg} = \qty[mode = math]{0.01}{cts/pix}$, a uniform exposure and a Gaussian PSF of width $\sigma_{PSF} = \qty[mode = math]{2}{pix}$, corresponding to the \enquote{Chandra} instrument scenario. The leftmost column shows the simulated counts data and the second most column from the left the underlying ground truth to compare. The other columns show the reconstructed flux by the different methods and prior assumptions for \jolideco. To enhance weak structures the images use an $\arcsinh$ stretching with a scale parameter of $a=0.02$. The stretching is the same for all images. The methods are described in detail in Section~\ref{subsec:methods}. A more detailed representation of the results is available at \url{https://jolideco.github.io/jolideco-comparison}.
            }
            \label{fig:comparison-scenarios}
        \end{centering}
    \end{figure*}
    %
    First we compare the performance of methods and prior assumptions on the different source scenarios. Figure \ref{fig:comparison-scenarios} shows the results for the selected subset of scenarios A1, B3, C3 and D4. 
    
    On scenario A1 all methods show acceptable to excellent results for the isolated point sources in the upper right of the image. This demonstrates all methods take into account the PSF and background model model correctly. The early stopped uniform prior still shows an extension for those cases, because the optimization process has not finished. Optimizing for 1000 iterations clearly improves the result and shows almost perfect point sources corresponding to the size of approximately one pixel. The \jolideco method with the Zoran-Weiss patch prior results in less sharp images, because the prior has been learned on natural images of landscapes and every day objects and scenes, which do not include point sources. The \jolideco method with the prior learned from GLEAM data, which also includes point sources, shows much improved results. However not as sharp as the results obtained with the uniform prior. In the lower left of the image the point sources become fainter and move closer, challenging all methods in succeeding to separate those as independent sources. The uniform prior optimized \endquote{to end} shows excellent results, still separating the sources. 
    
    For scenario B3 all method recover the point sources and the central extend source. However large difference can be observed in the reconstruction of the Gaussian shaped extended sources. As expected the uniform prior decompose the emission into many point sources, neglecting the spatial correlation between them. \textbf{Pylira} shows slightly improved results with the gaps between the bright emission peeks being more filled, however still lacks a convincing result into reconstructing the source as a single object. The \jolideco method with the patch priors shows the best results both visually as well as measured by the SSI. The GLEAM v0.1 patch prior reconstructs the sources almost perfectly, visually almost indistinguishable from the ground truth. The Zoran \& Weiss prior also shows good reconstruction of the extended sources, but less smooth and leaving a visible extension in the surrounding point sources.

    Scenario C3 challenges the methods to reconstruct a weak extended source under four brighter point sources. While again all methods reconstruct the point sources well, the visual differences are large for the weak underlying extended emission. The uniform prior with 1000 iterations decomposes the source into a grid of almost equidistant point sources resembling only the outer boundary of the original source. \textit{Pylira} shows a similar tendency but already improves the reconstruction of the extended source by leaving more emission in between the bright emission spots. The best results are achieved by the patch priors, which reconstruct a plausible extended source with approximately uniform brightness. 

    On the jet like features the patch prior does not show a preferred direction or bias in reconstruction and preserves the linear correlation between the pixels. 
    
    \jolideco shows excellent performance of a variety of morphological shapes. 
    it converges to the reference solution for very high signal to noise. We find the prior learned from GLEAM data to be a very good compromise between teh ability to separate nearby point sources and the ability to reconstruct the spatial correlation of extended sources.
    
    \subsubsection{Comparing Signal-to-Noise Scenarios}
    %
    \begin{figure*}
        \script{comparison-signal-noise.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/comparison-signal-noise.pdf}
            \caption{
                Comparison of the \jolideco method with the GLEAM v0.1 prior for different background levels and signal strength represented by scenario D1-5. The images are grouped by background levels with each column group showing the counts data in the left column and the reconstruction in the right column. The rows show the scenarios D1-5 as described in Section~\ref{subsec:test-datasets}. The background levels correspond to \qty[mode = text]{0.001}{cts / pixel}, \qty[mode = text]{0.01}{cts / pixel} and \qty[mode = text]{0.1}{cts / pixel}. The complete results for the simulations and additional information are available at \url{https://jolideco.github.io/jolideco-comparison}. \todo{the background handling does not yet seem right...maybe we should let the background norm free}
            }
            \label{fig:comparison-signal-noise}
        \end{centering}
    \end{figure*}
    %
    As a second experiment we explored the dependency of the results on the background level and signal strength. Figure~\ref{fig:comparison-signal-noise} shows the results for the \jolideco method with the GLEAM prior in the scenario D1-5. The background level was varied between 
    
    As expected the reconstruction quality improves with signal strength. This includes both the variation of the background as well as signal strength. \todo{sensitivity scales as approx signal / sqrt(Bkg)? } Visually the method converges to the ground truth for very high signal-to-noise ratio. Once the signal strength becomes very large there is no dependency of the result on the background level anymore (especially valid for scenario D5)

    We observe the method struggles to reconstruct a useful signal in the case of very high background (scenarios D1-3). 
    \todo{this is somewhat surprising as visually the signal can be seen in the data...at least for D2 and D3}.
    \todo{We should give a single number for the S/N, how to compute? Per source, like spiral ring and point sources...}    

    \subsubsection{Comparing Instrument Scenarios}
    %
    \begin{figure*}
        \script{comparison-instruments.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/comparison-instruments.pdf}
            \caption{
                Comparison of the \jolideco method with the GLEAM v0.1 prior for the different instrument scenarios. The images are grouped by instrument scenario with each column group showing the counts data in the left column and the reconstruction in the right column. The rows show the scenarios D1-5 as described in Section\ref{subsec:test-datasets}. 
                All results are available at \url{https://jolideco.github.io/jolideco-comparison}.
            }
            \label{fig:comparison-instruments}
        \end{centering}
    \end{figure*}
    %
    Finally we compare the results from the \jolideco method for the different instrument scenarios. Figure~\ref{fig:comparison-instruments} shows the data as well as flux images reconstructed using \jolideco and the GLEAM v0.1 patch prior. We consider the source scenarios D1-5 and the instrument scenarios \enquote{Chandra} and  \enquote{XMM} as well as the combination of their data to reconstruct a single image. 

    Using the data from the \enquote{Chandra} scenario alone results in a good reconstruction at all variations of scenario D. As expected the reconstruction quality improves for larger signal to noise ratio. 
    
    Using just the data from the \enquote{XMM} scenario results in lower quality reconstructions, due to the lower angular resolution. However we find that the spiral arm structure is reconstructed at all levels of signal strength, even when barely visible from the data by eye \todo{check saturation of the color map...}. The ring structure is sharpened considerably in all cases. The points source left and right to the spiral arm structure cannot be separated as individual point sources. This remains for all scenarios D1-5, as the brightness of the point sources remains the same. This is an inherent limitation of the method. \todo{how far are those apart compared to the PSF? Hoes does this compare to the point scenario with XMM?}.

    Combining the data results in the best reconstruction quality. The addition of the lower resolution \enquote{XMM} data does not negatively affect the quality of the reconstruction. When comparing the D5 scenario we see that the \enquote{Chandra} reconstruction shows a very good reconstruction of the spiral structure but still shows speckles in the surrounding ring. The latter suggests that the prior is not strong enough to guide the pixel to pixel correlations in the case of high signal strength. The strength of the prior could be increased with the $\beta$ parameter, but for this study we did not tweak individual parameters, but just used the default configuration. The speckle behaviour is not observed in the \enquote{XMM} case, where the reconstruction of the ring is smoother. In this case the correlation between pixels that are further apart is stronger because of the larger correlation PSF. However the reconstruction of the spiral structure in the center shows visual artifacts, with the individual arms not completely separated. The joint reconstruction shows the positive qualities of both individual reconstructions. This confirms the results from \cite{Ingaramo2014}, where the authors found that combining multiple measurements for deconvolution with RL to be a \textquote{general tool for combining images with complementary strengths}. \todo{not understood: reconstruction of point source becomes worse as spiral becomes brighter. Maybe because the strong signal dominates the likelihood function? Not sure how to improve this...is it image dynamics?}

    \section{Application Examples}
    \subsection{Deep \chandra Observation}
        
    \begin{figure*}
        \script{example-chandra.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/example-chandra.pdf}
            \caption{
               The \jolideco method applied to \chandra data of the SNR \textit{1E~0102.27219}. The left large image shows the summed counts from 25 observations. The right large image  shows the result from the \jolideco deconvolution process. The flux is given as counts relative to the reference observation with ID=1308. The panels \textit{Zoom A}, \textit{Zoom B} and \textit{Zoom C} show a zoomed-in version of three example regions in the SNR, marked with the white rectangles in the large images. The colormap and scale used in the zoom insets is the same as used for the larger images.
            }
            \label{fig:example-chandra}
        \end{centering}
    \end{figure*}

    As a first application example for real data we choose a set of short exposure observations of the SNR \textit{1E~0102.27219} by the \chandra telescope and combine them into a single deep observation using \jolideco and the patch prior learned from JWST data. In total we chose 25 observations\footnote{Same list of observation IDs as for \url{https://chandra.si.edu/photo/2009/e0102/}} taken with with the ACIS instrument and no grating. For \chandra the instrument response varies with the observation offset and energy \todo{reference}. With \jolideco we can handle  PSF for each observation. We assume a spatially constant background of \qty[mode = text]{1e-5}{cts/pixel} but allow the norm to vary later in the reconstruction process. We assume the exposure to be spatially constant and express its absolute value for each observation as the ratio of observation time with respect to a reference observation. As reference we chose the observation with id 1308.
    
    We reduced the data using \texttt{ciao} v4.15 and a custom \texttt{snakemake} workflow (see also Section~\ref{sec:reproducibility} on reproducibility). The PSF were simulated for each observation independently using the tool \texttt{marx} \todo{reference} and the \texttt{simulate\_psf} script. We selected the energy range from \qty[mode = text]{0.5}{keV} to  \qty[mode = text]{7}{keV} and a bins size of 0.25 native bins, corresponding to a bin size of \qty[mode = text]{0.001}{$\deg$/pixel} \todo{How large is the Chandra PSF compared to this?}. As reference spectrum for the PSF simulation we assumed the model derived by \cite{Plucinsky2017}. We converted the spectral from XSPEC to sherpa script using the helper script \texttt{convert\_xspec\_script}. As reported in \cite{Xi2019} the effect of pileup, even for the brightest regions is small (<5\%), so we did not include the effect of pileup in our PSF simulations. 

    For \jolideco we used the GMM prior learned on the JWST Cas A image. As there is a lot of statistics in the image we used and oversampling factor of 2. For each observation we allowed the background level and position calibration to vary. As reference for the positional shift we used the observation with the ID 1308. 
    Figure~\ref{fig:example-chandra} shows the final result. \jolideco clearly improves both the angular resolution as well as reduces the noise of the counts image. 

    \textbf{Zoom A} shows an example for a filamentous structure in more detail. \jolideco maintains the extended structure along the filament, while sharpening in the perpendicular direction. Looking at the upper right one can also see that weak background emission is suppressed, also improving the contrast of the image.
    
    \textbf{Zoom B} shows the region around the central compact object of the SNR \cite{Vogt2018}. While the emission is barely visible in the counts image it clearly sticks out above the background in the image reconstructed by \jolideco. The remaining visible extension in the point source is encoded in the JWST patch prior, which also shows a finite extension for point sources. As shown in Section~\ref{subsec:methods} for scenario A, the apparent size of the points source becomes smaller if the source becomes brighter.
    
    \textbf{Zoom C} shows a another circular filamentous structure of the SNR. In the counts data the structure is barely visible, while the image reconstructed by \jolideco shows a circular structure with a bright emission spot. 

    Checking the residuals per observation (not shown here), we noticed that the assumption of the time independence of the source as well the instrument response is not completely fulfilled in this case. We saw the detection efficiency of \chandra changed between the observations as well as sub-structure in the source expands over time. Both effects are previously known have been reported e.g. in \cite{Xi2019} and \cite{}. Despite the clear improvement in spatial resolution the proper motion of structures in the image might limit the quality of the result. The full analysis example is available at \url{https://github.com/jolideco/jolideco-chandra-examples}

    % \subsection{Combined \xmm and \chandra Observation}
    % As a second application example we combine data from \chandra and \xmm.
    % \url{https://github.com/jolideco/jolideco-chandra-nustar-examples}
        
    % \begin{figure*}
    %     \script{example-xmm-chandra.py}
    %     \begin{centering}
    %         \includegraphics[width=\linewidth]{figures/example-xmm-chandra.pdf}
    %         \caption{
    %             Chandra Example
    %         }
    %         \label{fig:example-xmm-chandra}
    %     \end{centering}
    % \end{figure*}


    \subsection{\fermi Event Types}
      \begin{figure*}
        \script{example-fermi-lat.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/example-fermi-lat.pdf}
            \caption{
                Illustration of the result of the \jolideco method applied to \fermi data of the supernova remnant \textit{RX J0852.0-4622} or \textit{Vela Junior}. The left image shows the counts above \qty[mode = text]{10}{GeV}. The different event classes are stacked into a single image. The image in the center shows the flux reconstructed by the \jolideco method. The image on the right shows the residuals as computed by Equation~\ref{eq:approx-sigma} and smoothed with a \textit{Gaussian} kernel of width 5~pixels. More information on this analysis example can be found on \url{https://github.com/jolideco/jolideco-fermi-examples}. \todo{Move color bar to top and make images larger}
            }
            \label{fig:example-fermi-lat}
        \end{centering}
    \end{figure*}
    As a second example we apply the \jolideco method to data from the Fermi Large Area Telescope (LAT). The LAT is a pair conversion \gammaray telescope on board of the Fermi satellite. It operates in the energy range between \qty[mode = text]{30}{MeV} to  \qty[mode = text]{300}{GeV}. For this example we chose again a Galactic source with complex morphology: the supernova remnant \textit{Vela Junior}. We use 14 years (from 2008 August 4 to 2023 August 4) of \texttt{P8R3\_SOURCE} photons with reconstructed energy in the range of \qty[mode = text]{10}{GeV} to  \qty[mode = text]{2}{TeV} and applied a zenith angle cut of \ang{105}. To preserve as much information as possible on the angular resolution of the instrument we split the data into the four event types \texttt{PSF0}, \texttt{PSF1}, \texttt{PSF2} and \texttt{PSF3}, as defined by the \fermi collaboration. We chose a pixel size of \qty[mode = text]{0.02}{$\deg$}, which corresponds to approximately 1/5th of the size of the PSF in the best event class. For the first part of the data reduction we use the standard \texttt{fermitools} \todo{add version and reference}.
    
    The \texttt{fermitools} output an energy dependent counts, exposure and PSF cube. To further reduce the data to images we use the \textit{Gammapy} package v1.0 \citep{GammapyZenodov1.0.1, Donath2023}. To compute an image of the expected background emission we rely on the latest Galactic diffuse emission model \texttt{gll\_iem\_v07.fits} and also include nearby sources from the 3FHL catalog \cite{Ajello2017}. We first fit the model components to the data using \textit{Gammapy}, excluding a circular region of radius \qty[mode = text]{1.02}{$\deg$} centered on \textit{Vela Junior}. Finally we reduce this model to an image by summing over the energy dimension. To compute the exposure and PSF image we assume a power law with index $\Gamma=1.77$ \citep{Ajello2017} as a spectral model for \textit{Vela Junior} and integrate over the energy dimension of the cube while weighting with the spectral model.
    
    We chose the GMM patch prior learned from \chandra SNRs described in the Section~\ref{sssec:chandra-snrs}. Because of the limited statistics we did not use any oversampling but keep the pixel size of the counts image. Figure~\ref{fig:example-fermi-lat} shows the data, result and residuals. The \fermi data is sparse, with very limited statistics. \jolideco clearly improves the noise level as well as angular resolution of the data. It reveals morphological details consistent with the HESS measurement \todo{Add HESS image or EROSITA for comparison? But this is not an analysis...}
    The calibration of the predicted counts finds different background model norms and positional shifts in order of \todo{add value} for the different event classes. This way it corrects for systematic errors in the reconstruction of the position. 
    
     To check the quality of the model we compute a map of approximate residual significance defined by:

    \begin{equation}
        \label{eq:approx-sigma}
        \sigma \approx \frac{\hat{N}_{\mathrm{Data}} - \hat{N}_{\mathrm{Pred}}} {\sqrt{\hat{N}_{\mathrm{Pred}}}}
    \end{equation}

    Where $\hat{N}$ is the weighted sum of counts from a Gaussian smoothing kernel.
    \todo{To account for the Poisson statistics it would be more precise to compute a TS map, but for an approximate estimation it should  be fine...I have a derivation for this formula }
    
    As a check we compute a map of residuals based on the predicted counts, the resulting distribution of residual significances per pixel is centered at $\mu=-0.001$ with a width of $\sigma=0.991$, indicating a close agreement with a unit \textit{Gaussian} distribution. More details of the example \fermi analysis are available at \url{https://github.com/jolideco/jolideco-fermi-examples}
    

    \section{Paper Reproducibility}
    \label{sec:reproducibility}
    In order to facilitate reproducibility of our results we provide the source code for the paper and figures at \url{https://github.com/jolideco/jolideco-paper}. The paper can be reproduced using using the tool \texttt{Showyourwork} \citep{Luger2021} and following the instructions in the \texttt{README.md} file in the same repository. All data is available for download from Zenodo \todo{how do we do this exactly...?} or can be reproduced using the custom  \texttt{Snakemake} workflows and repositories provided under the \jolideco GitHub organisation\footnote{\url{https://github.com/jolideco}}. For the Fermi-LAT data reduction we used a custom \texttt{Snakemake} workflow  available at\footnote{\url{https://github.com/adonath/snakemake-workflow-fermi-lat}}. For the Chandra data reduction we used a custom \texttt{Snakemake} workflow available at\footnote{\url{https://github.com/adonath/snakemake-workflow-chandra}}.
        
    
    \section{Summary \& Outlook}
    In this work we presented a new method for image deconvolution and denoising in the presence of Poisson noise. Using multiple observations of the same sky region the methods reconstructs a single flux image using the approximate MAP estimate of the joint likelihood under a patch based image prior. We showed that the methods leads to improved results in variety of scenarios. Especially it leads to much improved results for extended source with a size similar to the PSF. In this case it allows to reliably recover features, that are well beyond the angular resolution of the instrument.

    We showed that the approach of joining multiple observations leads improved results and allows to maximize the use of existing observational data.

    The choice of the prior becomes more important at low signal noise ratios. The right choice can yield superior reconstruction quality then any other method we have compared to. The advantage of the GMM patch prior is its expressiveness and that it is adapted to the data in the optimization process. However if the GMM patch prior is learned on unsuited data or data that not diverse enough it can contain a strong bias and there is the danger of hallucinating features. Testing of systematic uncertainties therefore becomes crucial.

    We consider the \jolideco method as a first step into the territory of \textit{ML inspired} methods for reconstruction of astronomical image data dominated by Poisson noise. While we achieved already superior results to any other existing method, the \jolideco methods shows large potential for future extension and improvements.
    
    The first category of future improvements is related to the prior handling. It is possibly to rely on more general mixture models such as a Student-T or a generalised Gaussian mixture model to model the distribution of patches. As shown by \cite{VanDenOord2014} those more flexible functional shapes can account for tails in the distributions and lead to improved results on natural images.
    
    To increase the size of the correlation window of the patch prior, \cite{Papyan2015} proposed to evaluate and combine the prior on patches extracted at multiple scales of the image. This is a promising approach to further improve the reconstruction quality for extended sources with low signal to noise ratio.
    
    To further improve the general expressiveness of the prior \cite{Altekrueger2022} used normalising flows (NF) to model the distribution of patches in the context of computer tomography image reconstruction. Just like a GMM the NF allows to evaluate its likelihood function and therfore construct an explicit, approximate \aposteriori probability for optimization. \todo{However it is also know that NFs struggle with multi-modality...not sure how this affects the propsoed approach...}  

    Another promising approach to handle prior information for images has been introduced by \citep{Ulyanov2017} with the so called \enquote{Deep Image Prior}. They showed that a prior structure for an image can be built into a deep neural network, where the architecture of the network encodes the prior information on the correlation structure between neighbouring pixels. The authors showed, that the deep image prior successfully imposes self similarity on the structures in the image. As structures in astronomical images often show sell-similarity this is and interesting approach, that could even be combined with the patch prior.
    
    We also plan to extend the method to deal with the spectral dimension of the data as well. We also plan to extend the method to handle multiple source components at the same time.
    
    \begin{itemize}
        \item Extend \jolideco to handle spectral dimension at the same time.
        \item Extend \jolideco to handle multiple source components at the same time.
        \item Exclude point sources either via separate component or J-invariance
    \end{itemize}

  

    We statistically combined data from existing telescopes to obtain a new \textit{virtual instrument}, with improved angular resolution and sensitivity. We invite the community of x-ray and \gammaray astronomers to use the method an combine data for new scientific discoveries, beyond the current angular resolution of existing instruments.

    \section*{Acknowledgements}
    This work was conducted under the auspices of the CHASC International Astrostatistics Center.
    CHASC is supported by NSF grants DMS-21-13615, DMS-21-13397, and DMS-21-13605; by the UK Engineering
    and Physical Sciences Research Council [EP/W015080/1]; and by NASA APRA Grant 80-NSSC21-K0285.
    
    We thank CHASC members for many helpful discussions, especially Xiao-Li Meng and Katy McKeough.
    DvD was also supported in part by a Marie-Skodowska-Curie RISE Grant (H2020-MSCA-RISE-2019-873089)
    provided by the European Commission.
    
    Aneta Siemiginowska, Vinay Kashyap, and Doug Burke further acknowledge support from NASA
    contract to the Chandra X-ray Center NAS8-03060.

    Some of the computations in this paper were conducted on the Smithsonian High Performance
    Cluster (SI/HPC), Smithsonian Institution \url{https://doi.org/10.25572/SIHPC}.

    \newpage
    \bibliography{bib.bib}
\end{document}
