% Define document class
\documentclass[twocolumn]{aastex631}

% Filler text
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{showyourwork}
\usepackage{amssymb}
\let\tablenum\relaxgit 
\usepackage{siunitx}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\arcsinh}{arcsinh}
\newcommand{\chandra}{\textit{Chandra}~}
\newcommand{\xmm}{\textit{XMM}~}
\newcommand{\fermi}{\textit{Fermi}-LAT~}
\newcommand{\jolideco}{\textit{Jolideco}~}
\newcommand{\aposteriori}{a~posteriori~}
\newcommand{\gammaray}{$\gamma$-ray\xspace}
\newcommand{\xray}{X-ray\xspace}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}

\newcommand{\vlk}[1]{{\color{blue} [VLK: #1]}}

% Begin!
\begin{document}

% Title
    \title{Joint Likelihood Deconvolution of Astronomical Images in the Presence of Poisson Noise}

% Author list
    \author[0000-0003-4568-7005]{Axel Donath}
    \author[0000-0002-0905-7375]{Aneta Siemiginowska}
    \author[0000-0002-3869-7996]{Vinay Kashyap}
    \author[0000-0000-0000-0000]{David van Dyk}


% Abstract with filler text
    \begin{abstract}
        We present a new method for Joint Likelihood Deconvolution (Jolideco) of a set of astronomical observations of the same sky region in the presence of Poisson noise.
        The method reconstructs a single flux image from a set of observations
        by optimizing the \aposteriori joint Poisson likelihood of all
        observations under a patch based image prior. The patch
        prior is parameterised by a standard Gaussian Mixture model (GMM), which is
        learned from astronomical images at other wavelengths.
        During reconstruction the GMM adapted to the structures
        in the data by finding the the most likely combination of 
        patches. 
        By applying the method to simulated data we show that
        the combination of multiple observations leads to an
        improved reconstruction quality in the regime with S/N ratio of.
        We show also that the method yields superior reconstruction quality
        to alternative standard methods such as the Richardson-Lucy method.
        We also show that the method can be highly adapted to various 
        analysis scenarios by learning problem specific GMM priors
        such as images of extragalactic or Galactic sources.
    \end{abstract}

    % Main body with filler text


    \section{Introduction}
    The angular resolution of any physical or astronomical imaging process is inherently limited by the instrument or telescope used.
    Furthermore, the quality of the resulting image is affected by background or instrumental measurement noise and non-uniform exposure. This is especially true in the low signal-to-noise regime, where images are affected by Poisson noise.
    
    Such as images taken by \xray and \gammaray~astronomy but also to other fields such as 

    To maximise the scientific use of available data it is desirable to correct for the instrumental imprint on the data. 
    
    We assume non-blind deconvolution, so the there is prior information of the point spread function (PSF). The PSF can be estimated empirically from observation of known point sources, such as distant activate galactic nuclei or a model of the PSF can obtained from simulation of the instrument. 
    
    This class of problems is called inverse problems.
    
    \subsection{Related Work}
    In literature there have been multiple efforts to correct the degrading of the images using statistical methods.
    The standard baseline method for image deconvolution was proposed independently by \cite{Richardson1972} and \cite{Lucy1974}. They used expectation maximisation to solve. We will refer to this method as "RL". 

    One of the significant challenges in image deconvolution is the problem of Rayleigh-Lynden Bell (RL) decomposition of structures into point sources. This problem was first introduced by Reeves et al. \cite{Reeves1994} and later analyzed by Fish et al. \cite{Fish1995}. Regularized RL methods, such as total variation \citep{Dey2006} and maximum entropy \citep{Bi1994}, have been proposed to overcome this issue.

    Multi-scale LIRA prior has been used in image deconvolution to incorporate error estimates and Bayesian sampling. This prior has been used in various studies, including those by Esch et al. \cite{Esch2004} and Connors et al. \cite{Connors2011}. However, the prior is ad-hoc and lacks physical information or assumptions, limiting its applicability.

    Another approach to image deconvolution is joint likelihood RL, which has been shown to be effective in creating a merged image from multiple images \cite{Ingaramo2014}. In this approach, RL takes the best out of each image and merges them to create a more accurate representation of the object.

    An alternative approach to solve the problem in the framework of information field theory. 
    
    Recently, image deconvolution using physical assumptions and decomposition into point and diffuse flux in 3D/4D has gained attention. Pumpe et al. \cite{Pumpe2018} and Selig et al. \cite{Selig2015} proposed priors based on physical assumptions, which have been shown to work well for large-scale emission. However, the smoothness assumption in these methods can lead to artifacts in the galactic plane.

    The challenge is due to fundamentally contradicting goals: deconvolution favoring point sources will decompose extend structures,
    while introducing local correlations will lead to 


    
    \begin{itemize}
         \item  Problem of RL decomposing structures into point sources: \cite{Reeves1994} \cite{Fish1995}
         \item regularised RL , e.g. total variation \citep{Dey2006} or maximum entropy \citep{Bi1994} \todo{other references?}
        \item Joint likelihood RL: \cite{Ingaramo2014} has found that RL take the best out of each image and create a "merged" image
        \item Multi-scale LIRA prior of course: \cite{Esch2004, Connors2011}, advantage of error estimates / Bayesian sampling. However prior is rather "ad-hoc", no physical information / assumptions. Just "smoothness"
        \item What abput d3po and d4po? Priors based on physical assumptions, decomposition into point and diffuse flux, but in 3d/4d \cite{Selig2015}\cite{Pumpe2018}. The smoothness assumption works for large scale emission, but leads to artifacts in the galactic plane... 
    \end{itemize}
    
    
    % \begin{figure*}
    %     \script{rl_decomposition.py}
    %     \begin{centering}
    %         \includegraphics[width=\textwidth]{figures/richardson-lucy-decomposition.pdf}
    %         \caption{
    %             Decomposition of the RL algorithm for various number of iterations. \todo{Improve figure}
    %         }
    %         \label{fig:rl_decomposition}
    %     \end{centering}
    % \end{figure*}

    \subsection{A note on Deep Learning Methods}
    In the past decade Machine learning (ML) especially deep convolutional neural networks became the \textit{de facto} standard
    for de-blurring and super-resolution of natural images \todo{reference review?}. However their success relies on the availability of massive sets of training data with typically low noise and corresponding ground truth to perform supervised learning. For a variety of reasons this approach is not suitable for astronomical x-ray and \gammaray data.
    
    Astronomical data is rare and expensive and there is no ground truth. One could rely on transfer learning and simulated data. E.g. radio or optical images could be forward folded with a known PSF model and be degraded with Poisson noise. However astronomical images have a large dynamic range and show a high diversity in morphological structure, which would still require al large amount of training data. However this approach has been successfully applied for specific classes of sources, such as images of Galaxies in the Sloan Digital Sky Survey (SDSS) \todo{reference} or as a post processing step of results obtained with regularised RL \todo{Reference by Starck et al.}.

    In addition to the challenge of obtaining training data, it is known that the PSF of instruments such as \chandra is highly variable. It varies with observation conditions and depends e.g. on energy as well as offset angle from the pointing direction. To cover the whole parameter space of the PSF one would need to train a network for each of the PSF models, which would require a very large computational effort.

    \cite{Li2014} have introduced a baseline architecture for a deep convolutional neural network for image deblurring.
    The convolutional nature of the network does not make it suitable for the task. As convolution introduces smoothing.
    However the network can learn an "inverse kernel", which needs to be larger because of the uncertainty principle (\todo{reference...}).
    The large kernels are introduced by separability assumption...
    In the limit of Poisson noise it is desirable to trace the full a-postiori likelihood, which is not possibly for most network architectures. Except for e.g. normalising flows \todo{there are references where NFs have bee used as priors...}

    \begin{itemize}
    \item diversity of astronomical images is high, one would need lots of training data
    \item Given a set of training images one could generate training data by forward folding with a known PSF model
    and adding Poisson noise
    \item however this would require training a new network for each PSF and noise level just to learn the task.
    This is a high computational effort. 
    \item By allowing multiple inputs one could still try to estimate a common flux image \todo{so far I have not heard
    about something like this...}
    \end{itemize}

   

    

    \section{Method}
    \subsection{Poisson Joint Likelihood}
    Our goal is to recover an image $\mathbf{x}$ from a set of multiple low counts observations. Most generally we assume our total dataset consists of $J$ individual observations of the same region of the sky, which are jointly modelled. The assumption is that the underlying \textit{true} emission image, we are looking for does not change with time. Under this assumption these datasets can be for example:

    \begin{itemize}
        \item Different observations of one instrument or telescope at different times and observation conditions. E.g., multiple observations of \chandra with different offset angles and exposure times.
        \item Observations of different telescopes, which operate in the same wavelength range. E.g., a \chandra
        and \xmm observation of the same region in the sky.
        \item A single observation of one telescope with different data quality categories and different associated instrument
        response functions. E.g., event classes for \fermi.
    \end{itemize}

    Or even an arbitrary combination of the possibilities listed above. 
    
    For each individual observation $j$ the predicted counts can be modelled by \textit{forward folding} the unknown flux image $\mathbf{x}$ through the individual instrument response:

    \begin{equation}
        \label{eq:model}
        \boldsymbol{\lambda}_j(\mathbf{x}) = \mathrm{PSF}_j \circledast \left(\mathbf{E}_j \cdot (\mathbf{x} + \mathbf{B}_j) \right)
    \end{equation}

    Where the expected counts $\lambda_j$ are given by the convolution of the unknown 
    flux image $\mathbf{x}$ with the observation specific point spread function $\mathrm{PSF}_j$. Additionally
    an observation specific image of the exposure $\mathbf{E}_j$ and background emission $\mathbf{B_j}$ can be
    optionally taken into account.

    Given a single observation $j$ of an unknown flux image
    $\mathbf{x}$ and assuming the noise in each pixel $i$ in the recorded counts image
    $\mathbf{D_j}$ follows a Poisson distribution, the likelihood $\mathcal{P}$
    of obtaining the measured image from a model image of the expected
    counts $\boldsymbol{\lambda_j}$ with $N$ pixels is given by:

    \begin{equation}
        \label{eq:poisson}
        P\left( \mathbf{D_j} | \boldsymbol{\lambda_j} \right) = \prod_{i=1}^N \frac{{e^{ - D_{j,i} } \lambda_{i,j} ^ {D_{j,i}}}}{{D_{j,i}!}}
    \end{equation}

    By taking the logarithm and dropping the constant terms one can transform the
    product into a sum over pixels, which is also often called the \textit{Cash}
    \citep{Cash1979} fit statistics:

    \begin{equation}
        \label{eq:cash}
        \mathcal{C}\left( \mathbf{D_j} | \boldsymbol{\lambda_j} \right) = \frac{1}{N}\sum_{i=1}^N \lambda_{j,i} - D_{j, i} \log{\lambda_{j,i}}
    \end{equation}

    To account for the possibility of different datasets having a different number of pixels we divide by $N$ (\todo{Does this need more justification...?}).
    By summing over all observations we finally get the joint log-likelihood of measuring a set of counts images $\mathbf{D}$ given an unknown flux image $\mathbf{x}$:

    \begin{equation}
        \label{eq:joint}
        \mathcal{L}\left( \mathbf{D} | \mathbf{x} \right) = \sum_{j=1}^J \mathcal{C}\left( \mathbf{D_j} | \mathbf{x} \right)
    \end{equation}

    Using Maximum Likelihood Estimation (MLE) we could try to get an estimate $\hat{\mathbf{x}}$ directly,
    using some optimization procedure. However in general this represents an \textit{ill posed}
    inverse problem. Estimates for $\mathbf{x}$ which have similar likelihood values $\mathcal{L}(\hat{\mathbf{x}_1})$ look very different from each other and might also look different compared to a given ground truth, if available. 
    Both the convolution operation as well as the level of noise lead
    to a loss of information in the data which cannot easily be recovered.
    The likelihood surface shows many local, close-by minima. Depending on the initial estimate for $\mathbf{x}$ and the minimization algorithm the MLE based estimate
    is likely to only find such a local minimum. This is fundamentally the reason for RL decomposing estimates into point sources, described in the previous section. 
    
    \todo{this sum corresponds to the loop over data sets in machine learning... }

    \subsection{A Posteriori Joint Likelihood}
    When reconstructing images in the context of inverse problems we are operating
    in a high dimensional parameter space, because each pixel in the image represents
    an independent parameter in the optimization process. By using prior information
    we can introduce correlations between the parameters, to reduce the \textit{effective}
    dimensionality of the problem and guide the optimization process towards unique 
    and more stable solutions.
    
    Using Bayes rule we can estimate the \aposteriori likelihood under a given prior:

    \begin{equation}
        \label{eq:bayes}
        P(\mathbf{x}|\textbf{D}) = P(\mathbf{x} ) \frac{P(\textbf{D} |\mathbf{x})}{P(\textbf{D})}
    \end{equation}

    And use a Maximum A Posteriori (MAP) approach to find an estimate for $\mathbf{x}$.
    Taking the logarithm of Eq.~\ref{eq:bayes}, replacing the definition of the likelihood
    $P(\mathbf{D}|\mathbf{x})$ with Eq.~\ref{eq:joint} and dropping the normalisation term $P(\mathbf{D})$
    which is independent of $\mathbf{x}$, we get the following expression for the log-posterior
    likelihood $\mathcal{L}$ :
    
    \begin{equation}
        \label{eq:total}
        \mathcal{L}\left(\mathbf{x} | \mathbf{D} \right) = \sum_{j=1}^J \mathcal{C}\left( \mathbf{D}_j | \mathbf{x} \right) - \beta \cdot \mathcal{P}(\mathbf{x})
    \end{equation}

    Where $\mathcal{C}\left( \mathbf{D_j} | \mathbf{x} \right)$ represents the summed log-likelihood
    for an individual observation $j$. Additionally the function includes a log-prior
    term $\mathcal{P}(\mathbf{x})$ and a factor $\beta$ to adjust the weight of the prior with respect to the joint log likelihood term. The parameter $\beta$ can be seen as a hyper-parameter, chosen by the user
    to quantify their belief in the correctness of the chosen prior distribution.
    \todo{this is somewhat dissatisfying...but in practice very common...see e.g. any kind of regularisation...if there is enough data cross validation could be used to tune this hyper-parameter}.

    \subsection{Patch Based Priors}
    In general it is difficult to capture global image statistics in a prior likelihood
    taking into account all pixel to pixel correlations. The size of the corresponding
    correlation matrix would grow with the number of pixels squared and thus quickly
    becomes computationally untraceable. However astronomical images typically only
    show local correlations between pixels. This is because the spatial
    scale of astrophysical processes in an image is always limited by the finite
    interaction time and the fact that the interaction cannot propagate
    faster than the speed of light. This means on smaller scales astronomical
    images often contain basic structures such as edges, corners, filaments or
    periodic patterns formed by local astrophysical processes.

    \todo{This is interesting, but move to intro...}
    \cite{Zoran2011} build on very similar assumption for natural images.
    Instead of assuming a fixed local correlation structures, such as smoothness
    they proposed to learn the local correlation structure on millions of patches
    extracted from natural images. 
    
    For this they proposed to split training images from representative
    data sets into small patches of size 8x8~pixels and fit a 64 dimensional
    Gaussian Mixture Model (GMM) to the distribution of extracted patches. Each pixel of the patch 
    is thereby treated as an independent dimension in the model. In total
    they chose to use $k=200$ Gaussian components.
    
    To use the GMM in reconstruction they introduce the expected patch likelihood (EPLL). 
    
    They showed that the patch based GMM prior led to much improved image reconstructions for multiple inverse problems such as de-noising, de-blurring and inpainting.

    Using half quadratic splitting and the Wiener filter solution, which is not possible
    in our case. Instead we choose a direct optimization by relying on differentiable
    programming later. 
    
    In practice we are interested in the likelihood of a given structure in a patch and
    So we subtract the mean value of a given patch. This represents the assumption that
    the image structure is independent of the total flux contained in the patch.
    
    One of the main advantages of the GMM is the possibility to evaluate its likelihood
    in closed form. For a given single patch $\mathbf{x}_n$ it is given by:

    \begin{equation}
        \label{eq:gmm}
        P_{GMM}(\mathbf{x}_n) = \sum_{k=1}^K \pi_k N(\mathbf{x}_n| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k^2)
    \end{equation}

    Where $N$ is a Gaussian distribution, $\boldsymbol{\mu}_k$ the vector of means for
    a given component $k$, $\boldsymbol{\Sigma}_k$ the corresponding covariance matrix
    and $\pi_k$ the weight the component. The different weights are required to sum to unity such that $\sum_{k=1}^N\pi_k = 1$. 

    From Equation~\ref{eq:gmm} we can also derive the log-likelihood for a specific choice of the component $k$:
        
    \begin{equation}
    \begin{split}
    \mathcal{P}_{GMM}(k| \mathbf{x}_n) = -\frac{1}{2} \left[ \right. d \log(2\pi)
    + \log(|\boldsymbol \Sigma_{k}|)\\
    + (\mathbf{x}_n - \boldsymbol{\mu}_{k})^T \Sigma^{-1}_{k}(\mathbf{x}_n - \boldsymbol{\mu}_{k}) \left. \right]
    \end{split}
    \end{equation}

    If we split the image into non-overlapping patches, we can find for each noisy patch $\mathbf{x}_n$ the GMM component $\hat{k}_n$, which maximises the log-likelihood of the GMM:
        
    \begin{equation}
        \hat{k}_n = \underset{k \in \{1, ..., N\} }{\argmax}{\mathcal{P}_{GMM}(k| \mathbf{x}_n)}
    \end{equation}
    \vspace{0.2em}

    As the number of components in the GMM is limited we can do this simply by evaluating the likelihood for each of the components and choose the one with
    the maximum value of the log-likelihood. To finally evaluate the log-likelihood
    of the total image we can sum up the log-likelihood of the individual patches: 

    \begin{equation}
        \mathcal{P}(\mathbf{x}, \mathbf{k}) = \sum_{n = 1}^N \mathcal{P}_{GMM}(\hat{k}_n | \mathbf{P}_n \mathbf{x})
    \end{equation}

    Where $\mathbf{P}_n$ is a matrix that extracts the $n$-th patch
    from the image $\mathbf{x}$ to be reconstructed. This way the index $k$ that identifies the GMM component becomes a hyper-parameter in the model and is optimized along with the image $\mathbf{x}$.
    
    \begin{itemize}
        \item the case of non-overlapping patches is rather simple
        \item for overlapping patches we have to make a few more approximations
        \item either we neglect the correlation between overlapping patches
        \item or we have to optimize the array of $\mathbf{k}$ at the same time, instead of "per patch optimization". However optimization on discrete 
        values is not easily possible in Pytorch. However here is one idea:
        group the GMM components by similarity and introduce a single continues
        parameter $z$, which interpolates between similar components.
        \item Another idea is to use "Gumbel Softmax"...
        \item we also compute the mean of the likelihood for the overlapping parts
    \end{itemize}

    To use the learned GMM as prior for the reconstruction of another image, the image is split into overlapping patches. For each of the patches the log-likelihood for each of the GMM components is evaluated. The resulting grid of overlapping patches is illustrated in Figure~\ref{fig:patches}.

    \begin{figure}
        \script{patches.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/patches.pdf}
            \caption{
                Grid of overlapping patches of size 8x8 pixels. The overlap size was chosen to be 2 pixels.
            }
            \label{fig:patches}
        \end{centering}
    \end{figure}

    \cite{Bouman2016} later adapted the patch prior reconstruction to be used
    with radio astronomy data.
    They have shown that the reconstructed image only weakly depends on the choice
    of the reference data on which the the patch prior is learned. They found
    equivalent results for GMMs learned on natural images and specifically
    simulated images of black hole ring structures.

    \subsubsection{Image Normalisation and Dynamic Range}
    Astronomical images typically show a much higher dynamic range compared to 
    natural images. This is mostly due to the existence of point sources, which
    rarely occur in natural images. Astronomical point sources typically relate
    to compact bright objects, such as active galactic nuclei at high distances,
    or binary objects and stars at Galactic distances.
    
    The high dynamic range of astronomical images challenges any deconvolution method.
    To recover point sources accurately it is required to keep correlations with
    neighbouring pixels low, while for extended source the opposite is true. Ideally
    those contradicting requirements can be handled with a single flexible prior assumption,
    which adapts to the data, such as the patch based GMM prior introduced in the previous
    section.

    \begin{equation}
        \tilde{\mathbf{x}}_n = f \left(\frac{\mathbf{x}_n - \Bar{\mathbf{x}}_n}{\Bar{\mathbf{x}}_n} \right)
    \end{equation}

    With a given non-linearity:

    \begin{equation}
        f(x) = \arcsinh{x / \alpha}
    \end{equation}

    \todo{The patch normalisation remains to be decided, but it would be advantageous to get rid of the hyper-parameter associated with the total image normalisation}.
    The GMM patch prior is learned from other images whose pixel intensities
    have no physical relationship of the intensities of the Poisson data we are
    dealing with. To overcome this the GMM is learned on normalized images,
    where the intensity values are constrained between 0 and 1.
    To evaluate the GMM patch prior on the 
    model image $\mathbf{x}$ the pixel values of $\mathbf{x}$  need to be mapped 
    in the same dynamic range between 0 and 1. Multiple common choices for such a
    mapping function are shown in Fig.~\ref{fig:image-norms}. 


    \begin{figure}
        \script{image-norms.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/image-norms.pdf}
            \caption{
                Various image norms with different dynamic behaviour.
            }
            \label{fig:image-norms}
        \end{centering}
    \end{figure}

    The choice of the image normalisation allows to adjust the contrast of the image
    and e.g. enhance low intensity structures, which also enhances the regularising
    effect of the patch prior. 

    \todo{Which normalisation to choose, atan, asinh, log, inverse-cdf?}

    There are arguments for $\arcsinh(\mathbf{x})$, see e.g. \cite{Lupton2004}, which we use.

    \subsubsection{Cycle Spinning}
    To avoid artifact due to the choice of the grid of overlapping patches we propose
    three variations:

    \begin{itemize}
        \item Cycle spinning by randomly shifting the image by a given number of pixels in x and y direction, e.g. used in \cite{Esch2004}
        \item Sub pixel cycle spinning, by randomly distributing the brightness of a given pixel to a 4 pixel grid \todo{This is something I came up with to allow for point sources to have sub-pixel positions, but I never showed it actually works...}
        \item A randomly chosen patch grid, such that each pixel is at least covered once.
        The random grid of patches was proposed by \cite{Parameswaran2018}. This has the advantage of potentially speeding up the computation as well. \todo{It is available in the implementation, but a bit "ad hoc", maybe just not mention...}
    \end{itemize}

    \subsection{Cross Validation}
    Cross validation (CV) is an established method for model selection in statistics
    and machine learning to prevent over-fitting the data. To perform CV the available
    data is slit into a "training" and "validation" dataset. While optimizing the
    log-likelihood value is monitored 
    
    The use of multiple dataset allows to use cross-validation to prevent overfitting.
    One of the standard techniques in "machine learning"...

    Cross validation is feasible once sufficient data is available, which is e.g. the 
    case for deep \chandra~observations.

    CV as stopping rule for RL was proposed by \cite{Reeves1995}...

    \subsection{Systematic Errors of Predicted Counts}
    Each measured counts image $\mathbf{D_j}$ is associated with systematic errors.
    This includes e.g. the pointing accuracy of the telescopes: when repeatedly pointed
    to the same object or when using different types of telescopes for observation, the
    position of an astronomical object might vary in the image between different
    observations. The process of correcting for these inaccuracies is called \textit{astrometry}.
    A second example of systematic errors is the variation of background emission
    between multiple observations.
    
    To account for both systematic effects we extend the model for the predicted
    counts introduced in Eq.~\ref{eq:model}, by a background normalisation factor
    $\alpha_j$ and a linear shift in position $\phi_j(\mathbf{x}| \delta_{x,j}, \delta_{y,j})$,
    which is specific for each observation. This yields the following modified model definition:
    
    \begin{equation}
        \label{eq:model-npred-calibration}
        \mathbf{\lambda}_j = \mathrm{PSF}_j \circledast \left(\mathbf{E}_j \cdot (\phi_j(\mathbf{x}| \delta_{x,j}, \delta_{y,j}) + \alpha_j \cdot \mathbf{B}_j) \right)
    \end{equation}
    
    In practice one observation is used as a reference and its corresponding
    systematic shifts in position $\delta_{x,j}, \delta_{y,j}$ are frozen during
    optimization. An example of the effect of the positional shift and background
    normalisation can be seen in Fig.~\ref{fig:npred-systematics}.

    \subsubsection{Statistical Errors}
    \begin{itemize}
        \item Bootstrapping observations
        \item Bootstrapping events
        \item Hessian diagonal
    \end{itemize}
    
    \section{Implementation}
    \subsection{Jolideco Framework}
    The goal of the reconstruction process is to optimize the a-posteriori
    likelihood defined by Equation~\ref{eq:total}. Given that each pixel
    in the reconstructed image represents an independent parameter
    this represents a high dimensional optimization problem.
    Differentiable programming frameworks such as \texttt{PyTorch}~\citep{Pytorch2019}
    allow for solving these kind of high dimensional modeling problems, by using
    back-propagation and adapted optimization methods for high dimensional
    models such as stochastic gradient decent.

    We implemented the \jolideco method as an independent Python package, 
    based on \texttt{PyTorch} as optimization back-end. We tried to define a modular,
    object oriented code structure, to allow parts of the algorithm to be
    flexible and interchangeable, such as choice of image normalisation scales,
    the choice of GMM models, optimization methods and serialisation formats for the 
    reconstruction results as well as corresponding diagnostic information
    such as the trace of the posterior and prior and likelihood values.
    The package is available at \url{https://github.com/jolideco/jolideco}

    We also use \texttt{Numpy}~\citep{Numpy2020} for handling data arrays and
    \texttt{Matplotlib}~\citep{Hunter2007} for plotting.
    To learn the Gaussian Mixture Models from patches we the standard implementation
    provided in the \texttt{Scikit-Learn}~\citep{Skimage2014} package. To handle the 
    serialisation of images to the FITS data format and to handle world coordinate
    systems (WCS) we use \texttt{Astropy}~\citep{Astropy2018}.
    For optimization and internal data handling we use \texttt{PyTorch}.

    \subsection{Jolideco GMM Library}
    \label{ssec:jolideco-gmm-library}
    On radio data \cite{Bouman2016} found the results to be mostly independent of the choice of data the GMM prior was learned on. The strength of the GMM prior is its high \textit{expressiveness}, meaning the prior contains a large variety of patch shapes and adapts to the data in the optimization process. However in the low signal-to-noise regime we typically expect a higher dependence of the results on the choice of the data the GMM was learned on. The choice of the prior becomes more important and to avoid introducing a strong bias it should be adapted to the analysis scenario. For this reason and for purposes of testing we learned and provide a selection of GMMs (a \textit{Library}) to be used with the patch prior for download in a \textit{GitHub} repository\footnote{\url{https://github.com/jolideco/jolideco-gmm-library}}.
    
    \subsubsection{Zoran \& Weiss}
    As baseline reference GMM we use the original model provided by \cite{Zoran2011}. The model was learned from the Berkley image database, which contains natural images. The model was learned by splitting the images into 8x8 patches and provided by the authors as part of the source code for download\footnote{\url{https://people.csail.mit.edu/danielzoran/epllcode.zip}}.
    More details of how the model was learned are given in the original publication.
    
    \subsubsection{GLEAM Radio Data}
    \label{sssec:gleam-radio-data}
    
    A prior learned from astronomical data for analysis of Galactic sources with 
    rich morphology, such as supernova remnants, pulsar winds etc., but also include point sources. A good "all purpose" prior.
    High signal to noise Radio data from \cite{HurleyWalker2022}.
    There is physical correlation between e.g. the very high energy gamma range
    and radio as emission is produced from the same electron populations.
    We downloaded the GLEAM data using the Gleam VO Client\footnote{\url{https://github.com/ICRAR/gleamvo-client}}. We restricted the choice
    of data to the Galactic Plane between $b=\pm10\deg$ and $\ell = \pm180\deg$.
    In total we extracted \todo{insert number} patches and trained a GMM with 128
    components. 
    

    \subsubsection{NRAO Jets, Jets, Jets}
    \todo{Include the jet prior or not?}
    For the specific application of jet detection and analysis in extract galactic data we learned a GMM patch prior from radio jet images. It represents the prior knowledge of both the morphology and preferred direction of the jet. As reference data we chose
    a PR image released under the name \textit{Jets, jets jets}  by the NRAO collaboration containing ~150 example jets from their data\footnote{\url{https://public.nrao.edu/gallery/quicklook-samples/}}.

    We first split the compiled image into 150 individual examples. For each image we ran a blob detection algorithm and line fit to find the preferred direction of the jet. Using this information we aligned all images to the same horizontal reference direction of the jet split all images into patches of size 8x8 pixels. In total we extracted \todo{insert number} patches and used those to train a GMM with 32 components. The prior is useful in a scenario were the direction of the jet known from e.g. previous radio observations. 

    Alternatively we used a \textit{data augmentation} approach to learn a GMM prior with a randomized direction of the jet. For this we rotated the jet image after alignment by 
    a random angle. Then again we split each image into patches of size 8x8 pixels. In total we created \todo{insert number} patches and learned a GMM with 64 components. The prior
    is adapted to the analysis scenario of jet detection, where no prior knowledge on the
    direction is available.

    \begin{figure}
        \script{gmm-eigen-images.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/gmm-eigen-images.pdf}
            \caption{
                Eigenimages of the covariance matrices of the 32 components of the GMM learned from the NRAO jets. \todo{Provide other examples...?!?}
            }
            \label{fig:gmm-eigen-images}
        \end{centering}
    \end{figure}

    \subsubsection{Chandra SNRs}
    \label{sssec:chandra-snrs}. Because of the limited statistics we did not use any oversampling but kept the pixel
    Astrophysical processes forming the morphological structures are typically self-similar \todo{provide physical argument...I guess it is just the distance distribution?}. Structures appear on multiple angular scales. This means the prior could even be learned from the data itself if sufficiently free of noise. For analysis of Galactic sources with complex morphology and typically radially symmetric features such as supernova remnants (SNRs) we learned a prior from images of the bright \chandra SNRs \textit{Cas A} and \textit{G292.0+01.8}. In total we extracted \todo{insert number} patches and learned a GMM with 128 components.


     
    \subsection{Computational Performance}
    We conduct as series of performance benchmarks (see Appendix?) to asses the scalability of the method to a large number of observations
    as well as large images. 

    \begin{itemize}
        \item Results are available in \url{https://github.com/jolideco/jolideco-benchmarks}
        \item We tried the binary search tree as proposed in \cite{Parameswaran2018},
        but without success, most likely because the native \texttt{Pytorch} support
        for parallelisation speeds up the standard GMM quite a bit...
        \item Could use a Neural Net for GMM component selection as proposed in \cite{Rosenbaum15}
        \item GPU support via \texttt{Pytorch}

    \end{itemize}

    \subsection{Optimization Strategy}
    Maximising the \aposteriori likelihood defined Eq.\ref{eq:total} with respect to the model parameters $\mathbf{x}$ is a high dimensional optimization problem. To improve the convergence rate of the optimization process we employ the following optimization strategy: the optimization strategy begins with the optimizer viewing $\log{\mathbf{x}}$, and the first step involves running a rough estimate using a uniform prior. This step is optional, but if chosen, the next step involves optimizing the parameters associated with the systematic error described in Eq.~\ref{eq:model-npred-calibration}. Following this, a full optimization is performed using the patch prior, which can be computationally expensive. The optimization algorithm used is ADAM, a stochastic gradient descent method, without decay. Finally, there is an optional step of learning rate scheduling, although so far, a fixed step size value of $1e-3$ has been used.
    
    In practice we employ the following optimization strategy:
    \begin{itemize}
        \item the optimizer sees $\log{\mathbf{x}}$
        \item First run with uniform prior, to get a rough estimate
        \item (optional) Then optimize the parameters associated to the systematic error described in Eq.~\ref{eq:model-npred-calibration}.
        \item Then we go for a full optimization using the patch prior, which can be computationally costly
        \item we use ADAM (stochastic gradient decent), without decay
        \item  (optional) learning rate scheduling? So far just a fixed step size value of $1e-3$
    \end{itemize}
    
    
        
    \section{Experiments}
    \subsection{Test Datasets}\label{sec:testmodels}
    \todo{Vinay writes this?}
    \vlk{yes}

    \vlk{We have devised four distinct arrangements of combinations of point and extended sources of differents shapes to test the algorithms.  The arrangements are 
    \begin{itemize}
        \item[(A)] {\bf asterism:} A set of four point sources arranged around an extended Gaussian source
        \item[(B)] {\bf points:} Point sources arranged with different separations
        \item[(C)] {\bf shield:} Disk shaped flat extended source with superposed point sources and linear jet-like features
        \item[(D)] {\bf spiral:} Extended thin double-spiral with a flat disk at the center and point sources adjacent to it
    \end{itemize}
    The arrangements are shown in Figure~TBD, and the details of the locations and brightness of each component is listed in Table~TBD.}

    \begin{figure*}
        \centering\includegraphics{figures/chandra_gauss_fwhm4710_128x128_aster1+point1+disk1+spiral3.pdf}
        \caption{Illustrating the source patterns used to make test datasets.  The patterns shown are for the {\sl (A)} {\tt asterism}, {\sl (B)} {\tt points}, {\sl (C)} {\tt shield}, and {\sl (D)} {\tt spiral} cases (see Section~\ref{sec:testmodels}), convolved with a sharp PSF (Gaussian with $\sigma=2$~pix).  All images are of size $128{\times}128$.  The dashed red and magenta boxes depict $32{\times}32$ and $64{\times}64$ regions of interest.  Note that the boxes are displaced to the lower left corner in Case B, and leftwards in Case D.}
        \label{fig:simulated_models_chandra}
    \end{figure*}
     
    \begin{figure*}
        \centering\includegraphics{figures/xmm_gauss_fwhm14130_128x128_aster1+point1+disk1+spiral3.pdf}
        \caption{As in Figure~\ref{fig:simulated_models_chandra}, convolved with a diffuse PSF (Gaussian with $\sigma=6$~pix).}
        \label{fig:simulated_models_xmm}
    \end{figure*}

    We use the test datasets provided by Vinay et al.
    First we evaluate the performance of the method on a set of simulated observations.
    For this we assume:

    \begin{itemize}
        \item An instrument with good angular resolution, but low effective area (e.g. like Chandra)
        \item An instrument with worse angularr resolution, bur higher effective area (e.g. like XMM)
    \end{itemize}

    For both scenarios we assume a Gaussian PSF of sizes $\sigma = 2$ pixels and  $\sigma = 5$.
    As true flux we consider the following scenarios:

    \begin{itemize}
        \item Point source with varying distances and brightness
        \item Disk and point sources of varying brightness
        \item Spiral and point sources of varying brightness
    \end{itemize}

    Background levels of $\lambda_{Bkg}= 0.001, 0.01 \textrm{and} 0.1 \textrm{counts/pixel}$. 
    
    Show results of experiments on simulated toy datasets

    \subsection{Methods}
    We compare the performance of 5 different methods:

    \begin{itemize}
        \item Pylira \citep{Esch2004, scipy_proceeding}
        \item \jolideco with a uniform prior and 10 iterations. This can be considered as a reference similar to RL.
        \item \jolideco with a uniform prior and 10 iterations. This can be considered as a reference similar to RL.
        \item \jolideco with a patch prior learned on Gleam data and sum patch norm
        \item \jolideco with a patch prior from \cite{Zoran2011} and the mean subracted patch norm. This is for comparison to the original method and the method used on \cite{Bouman2016}.
    \end{itemize}
    

    \subsection{Results}
    All the results are available at \url{https://jolideco.github.io/jolideco-comparison}.
    \subsubsection{Comparing Scenarios}
    \jolideco shows excellent performance of a variety of morphological shapes. 
    it converges to the reference solution for very high signal to noise.

    \begin{figure*}
        \script{comparison-scenarios.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/comparison-scenarios.pdf}
            \caption{
                Comparison of different deconvolution methods for different scenarios. The left column shows the raw simulated data and the second most column from the left the ground truth to comapare to. The simulation used a background level of "bg1 and a Gaussian PSF of \todo{x pixels}. The \textit{Pylira} method uses the \textit{Pylira} method with standard configuration.
                All results are available at \url{https://jolideco.github.io/jolideco-comparison}.
            }
            \label{fig:comparison-scenarios}
        \end{centering}
    \end{figure*}

    \subsubsection{Comparing Background Levels}
       \begin{figure*}
        \script{comparison-signal-noise.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/comparison-signal-noise.pdf}
            \caption{
                Comparison of Jolideco (GLEAM v0.1) for different signal to noise ratios.
                All results are available at \url{https://jolideco.github.io/jolideco-comparison}.
            }
            \label{fig:comparison-signal-noise}
        \end{centering}
    \end{figure*}

    \subsubsection{Comparing Instruments}

    \begin{figure*}
        \script{comparison-instruments.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/comparison-instruments.pdf}
            \caption{
                Comparison of different deconvolution methods for different scenarios. The left column shows the raw simulated data and the second most column from the left the ground truth to comapare to. The simulation used a background level of "bg1 and a Gaussian PSF of \todo{x pixels}. The \textit{Pylira} method uses the \textit{Pylira} method with standard configuration.
                All results are available at \url{https://jolideco.github.io/jolideco-comparison}.
            }
            \label{fig:comparison-instruments}
        \end{centering}
    \end{figure*}

    \section{Application Examples}
    \subsection{Deep \chandra Observation}
    As a first application example we choose a set of short exposure observations of the SNR \textit{E0102} by the \chandra telescope. \chandra is a space-based X-ray observatory that has been operational since 1999. It comprises nested cylindrical paraboloid and hyperboloid surfaces that create an imaging optical system for X-rays. In the focal plane, it has multiple instruments designed for various scientific purposes, such as the high-resolution camera (HRC) and the Advanced CCD Imaging Spectrometer (ACIS). Chandra has a typical angular resolution of 0.5 arcseconds and covers the energy range from \qty[mode = text]{0.1}{keV} to  \qty[mode = text]{10}{keV}. 
    
    In total we choose 25 observations with the ACIS instrument and no grating. For \chandra the instrument response varies with the observation offset \todo{reference}. With \jolideco we can handle the correct PSF for each observation. We assume a spatially constant background and exposure, however the exposure is scaled to a reference observation. The data reduction was done using ciao v4.15 and the custom snakemake workflow. PSFs were simulated for each observation independently using \textit{marx} \todo{reference} and the \textit{simulate\_psf} script. List of observation IDs can be found online. Energy range was selected from \qty[mode = text]{0.5}{keV} to  \qty[mode = text]{7}{keV} and a bins sice of 0.5 "native" bins, corresponding to a bin size of \qty[mode = text]{0.001}{}. For the spectrum we fitted a model a circular region \todo{which one?} and used it for the PSF simulation. 

    For \jolideco we used the GMM prior learned on Chandra SNRs and an oversampling factor of 2. For each observation we allow the background level and position calibration to vary. Figure~\ref{fig:example-chandra} shows the final result. \jolideco clearly improve the angular resolution.

    To check the quality of the model we compute a map of approximate residual significance defined by:

    \begin{equation}
        \label{eq:approx-sigma}
        \sigma \approx \frac{\hat{N}_{\mathrm{Data}} - \hat{N}_{\mathrm{Pred}}} {\sqrt{\hat{N}_{\mathrm{Pred}}}}
    \end{equation}

    Where $\hat{N}$ is the weighted sum of counts from a Gaussian smoothing kernel.
    \todo{To account for the Poisson statistics it would be more precise to compute a "TS" map, but for an approximate estimation it should  be fine...I have a derivation for this formula }
    
    \url{https://github.com/jolideco/jolideco-chandra-examples}

    
    \begin{figure*}
        \script{example-chandra.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/example-chandra.pdf}
            \caption{
                Chandra Example
            }
            \label{fig:example-chandra}
        \end{centering}
    \end{figure*}


    \subsection{Combined \xmm and \chandra Observation}
    As a second application example we combine data from \chandra and \xmm.
    \url{https://github.com/jolideco/jolideco-chandra-nustar-examples}
        
    \begin{figure*}
        \script{example-xmm-chandra.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/example-xmm-chandra.pdf}
            \caption{
                Chandra Example
            }
            \label{fig:example-xmm-chandra}
        \end{centering}
    \end{figure*}


    \subsection{\fermi Event Types}
    As a last application example we apply the \jolideco method to data from the Fermi Large Area Telescope (LAT).
    The LAT is a pair conversion gamma-ray telescope on board of the Fermi satellite. It operates in the energy range between \qty[mode = text]{30}{MeV} to  \qty[mode = text]{300}{GeV}. For the example we chose again a Galactic source with complex morphology: the supernova remnant \textit{Vela Junior}. We use 14 years (from 2008 August 4 to 2023 August 4) of \texttt{P8R3\_SOURCE} photons with reconstructed energy in the range of \qty[mode = text]{10}{GeV} to  \qty[mode = text]{2}{TeV} and applied a zenith angle cut of \ang{105}. To preserve as much information as possible on the angular resolution of the instrument we split the data into the four event types \texttt{PSF0}, \texttt{PSF1}, \texttt{PSF2} and \texttt{PSF3}, as
    defined by the \fermi collaboration. We chose a pixel size of \qty[mode = text]{0.02}{$\deg$}, which corresponds to approximately 1/5th of the size of the PSF in the best event class. For the first part of the data reduction we use the standard \textit{fermitools} \todo{add version and reference}.
    
    As the \textit{fermitools} output an energy dependent counts, exposure and PSF cube, we use the \textit{Gammapy} package to further reduce the data. In a first step we compute an image of the expected background emission. For this we rely on the latest Galactic diffuse emission model \texttt{gll\_iem\_v07.fits} and nearby sources from the 3FHL catalog \cite{Ajello2017}. We fit the model components to the data, excluding a circular region of radius \qty[mode = text]{1.02}{$\deg$} centered on \textit{Vela Junior}. Finally we reduce this model to an image by summing over the energy dimension.
    
    Assuming a power law with index $\Gamma=1.77$ \citep{Ajello2017} as a spectral model for \textit{Vela Junior} we also compute an effective PSF and exposure image by integrating over the energy dimension of the cube and weighting with the spectral model.
    
    We chose the GMM patch prior learned from \chandra SNRs described in the Section~\ref{sssec:chandra-snrs}. Because of the limited statistics we did not use any oversampling but kept the pixel size of the counts image.
    
    Figure~\ref{fig:example-fermi-lat} shows the data, result and residuals. The \fermi data is sparse, with very limited statistics.
    \jolideco clearly improves the noise level as well as angular resolution. It reveals morphological details consistent with the HESS measurement \todo{Add HESS image or erosita for comparison? But this is not an analysis...}
    The calibration of the predicted counts finds different background model norms and positional shifts in order of \todo{add value} for the different event classes. This way it corrects for systematic errors in the reconstruction of the position. As a check we compute a map of residuals based on the predicted counts, the resulting distribution of residual significances per pixel is centered at $\mu=-0.001$ with a width of $\sigma=0.991$, indicating a close agreement with a unit \textit{Gaussian} distribution.
    

    \url{https://github.com/jolideco/jolideco-fermi-examples}
    
    \begin{figure*}
        \script{example-fermi-lat.py}
        \begin{centering}
            \includegraphics[width=\linewidth]{figures/example-fermi-lat.pdf}
            \caption{
                The \jolideco method applied to \fermi data of the supernova remnant \textit{RX J0852.0-4622} or \textit{Vela Junior}. The left image shows the counts above \qty[mode = text]{10}{GeV}. The different event classes are stacked into a single image.
                The image in the center show the flux reconstructed by the \jolideco methods. The image on the right shows the 
                residuals as compute by Equation~\ref{eq:approx-sigma} and smoothed with a \textit{Gaussian} kernel of width 5~pixels. More information on this analysis example can be found on \url{https://github.com/jolideco/jolideco-fermi-examples}. \todo{Move color bar to top and make images larger}
            }
            \label{fig:example-fermi-lat}
        \end{centering}
    \end{figure*}

    \section{Paper Reproducibility}
    In order to facilitate reproducibility we make source code for the paper and figures  available at \url{https://github.com/jolideco/jolideco-paper}.
    The paper can be reproduced using using the tool \texttt{Showyourwork} \citep{Luger2021}. See instructions 
    All data is available on Zenodo \todo{how do we do this exactly...?}
    
    For the Fermi-LAT data reduction we used a custom \texttt{Snakemake} workflow \footnote{\url{https://github.com/adonath/snakemake-workflow-fermi-lat}}.

    For the Chandra data reduction we used a custom \texttt{Snakemake} workflow \footnote{\url{https://github.com/adonath/snakemake-workflow-chandra}}.
        
    
    \section{Summary \& Outlook}
    In this work we presented a new method for image deconvolution and denoising in the presence of Poisoon noise.
    Jolideco is great...

    We consider the \jolideco method as a first step into the territory of machine learning inspired methods for image data 
    dominated by Poisson noise. By applying patch based transfer learning. 

    While we achieved already superior results to any other method we have identified multiple directions for improvement and variations of the method. Firstly it is possibly to rely on more general mixture models such as a Student-T or a generalised Gaussian mixture model. Those more flexible functional shapes can account for tails in the distributions. Improved results
    on natural images have been reported e.g. by \cite{}.

    To further improve the expressiveness of the prior \cite{Altekrueger2022} used normalising flows to model the distribution of patches.

    
    
    \begin{itemize}
        \item Extend \jolideco to handle spectral dimension at the same time.
        \item one could try deep image priors \citep{Ulyanov2017}
        \item Extend to use GMM patch prior on multiple scales, see \cite{Papyan2015}
        \item Exclude point sources either via separate component or "J-invariance"
    \end{itemize}

    We invite the community of x-ray and \gammaray astronomers to use the method on combined data. 

    \section*{Acknowledgements}
    This work was conducted under the auspices of the CHASC International Astrostatistics Center.
    CHASC is supported by NSF grants DMS-21-13615, DMS-21-13397, and DMS-21-13605; by the UK Engineering
    and Physical Sciences Research Council [EP/W015080/1]; and by NASA APRA Grant 80-NSSC21-K0285.
    
    We thank CHASC members for many helpful discussions, especially Xiao-Li Meng and Katy McKeough.
    DvD was also supported in part by a Marie-Skodowska-Curie RISE Grant (H2020-MSCA-RISE-2019-873089)
    provided by the European Commission.
    
    Aneta Siemiginowska, Vinay Kashyap, and Doug Burke further acknowledge support from NASA
    contract to the Chandra X-ray Center NAS8-03060.

    Some of the computations in this paper were conducted on the Smithsonian High Performance
    Cluster (SI/HPC), Smithsonian Institution \url{https://doi.org/10.25572/SIHPC}.

    \newpage
    \bibliography{bib.bib}
\end{document}
